{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Output Parser é€²éš RAG æ‡‰ç”¨\n",
    "\n",
    "## èª²ç¨‹ç›®æ¨™\n",
    "- æŒæ¡æ¨¡å‹é‡åŒ–æŠ€è¡“ï¼ˆ4-bit quantizationï¼‰\n",
    "- å­¸ç¿’ LangChain Output Parser å„ç¨®é¡å‹\n",
    "- å¯¦ä½œçµæ§‹åŒ– RAG è¼¸å‡ºç³»çµ±\n",
    "- æ‡‰ç”¨æ–¼å¯¦éš›å•†æ¥­å ´æ™¯\n",
    "\n",
    "## æ•™å­¸å¤§ç¶±\n",
    "1. ç’°å¢ƒè¨­å®šèˆ‡é‡åŒ–æŠ€è¡“\n",
    "2. Output Parser åŸºç¤\n",
    "3. RAG + Parser æ•´åˆ\n",
    "4. å•†æ¥­æ‡‰ç”¨æ¡ˆä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ç’°å¢ƒè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: å®‰è£å¿…è¦å¥—ä»¶\n!pip install -q langchain-huggingface langchain-community langchain\n!pip install -q transformers accelerate bitsandbytes\n!pip install -q pypdf faiss-cpu==1.10.0\n!pip install -q pydantic\n!pip install -q rank-bm25  # BM25 æª¢ç´¢\n!pip install -q sentence-transformers  # Reranker\n\nprint(\"âœ… å¥—ä»¶å®‰è£å®Œæˆ\")\nprint(\"âš ï¸  è«‹é‡å•Ÿ Runtime å¾Œå†åŸ·è¡Œå¾ŒçºŒ cells\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: HuggingFace èªè­‰\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "token = userdata.get('HF_TOKEN')\n",
    "login(token=token)\n",
    "\n",
    "print(\"âœ… HuggingFace èªè­‰æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: æ›è¼‰ Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# åˆ‡æ›åˆ°è³‡æ–™ç›®éŒ„\n",
    "%cd drive/MyDrive/data_rag\n",
    "\n",
    "print(\"âœ… Google Drive æ›è¼‰æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: å°å…¥åŸºç¤å¥—ä»¶\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import (\n",
    "    StrOutputParser,\n",
    "    JsonOutputParser,\n",
    "    PydanticOutputParser,\n",
    "    CommaSeparatedListOutputParser\n",
    ")\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict\n",
    "import json\n",
    "\n",
    "print(\"âœ… å¥—ä»¶å°å…¥å®Œæˆ\")\n",
    "print(f\"ğŸ”§ PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"ğŸ® CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ“Š GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: é‡åŒ–æŠ€è¡“å¯¦æˆ°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: è¨­å®š 4-bit é‡åŒ–é…ç½®\n",
    "print(\"âš™ï¸  è¨­å®š 4-bit é‡åŒ–é…ç½®...\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # å•Ÿç”¨ 4-bit é‡åŒ–\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # ä½¿ç”¨ bfloat16 è¨ˆç®—\n",
    "    bnb_4bit_quant_type=\"nf4\",             # NormalFloat 4-bit é‡åŒ–é¡å‹\n",
    "    bnb_4bit_use_double_quant=True,        # é›™é‡é‡åŒ–ï¼Œé€²ä¸€æ­¥å£“ç¸®\n",
    ")\n",
    "\n",
    "print(\"\"\"\\nğŸ“˜ é‡åŒ–åƒæ•¸èªªæ˜ï¼š\n",
    "- load_in_4bit: å°‡æ¨¡å‹æ¬Šé‡é‡åŒ–ç‚º 4-bitï¼ˆåŸæœ¬ 16-bitï¼‰\n",
    "- bnb_4bit_compute_dtype: è¨ˆç®—æ™‚ä½¿ç”¨çš„è³‡æ–™å‹åˆ¥\n",
    "- bnb_4bit_quant_type: NF4 (NormalFloat) é©åˆç¥ç¶“ç¶²è·¯\n",
    "- bnb_4bit_use_double_quant: é‡åŒ–å¸¸æ•¸ä¹Ÿé€²è¡Œé‡åŒ–\n",
    "\n",
    "ğŸ’¡ é æœŸæ•ˆæœï¼š\n",
    "- è¨˜æ†¶é«”ä½¿ç”¨æ¸›å°‘ç´„ 75%\n",
    "- 1B æ¨¡å‹å¾ ~2GB é™è‡³ ~0.5GB\n",
    "- æ¨ç†é€Ÿåº¦ç•¥æœ‰æå‡\n",
    "- ç²¾åº¦æå¤±æ¥µå°ï¼ˆ<1%ï¼‰\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: è¼‰å…¥é‡åŒ–æ¨¡å‹\n",
    "print(\"ğŸš€ è¼‰å…¥é‡åŒ–æ¨¡å‹ gemma-3-1b-it...\\n\")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/gemma-3-1b-it\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "print(\"\\nâœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼\")\n",
    "print(\"\\nğŸ’¾ è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ï¼š\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"å·²ä¿ç•™: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: æ¸¬è©¦é‡åŒ–æ¨¡å‹\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "print(\"ğŸ§ª æ¸¬è©¦é‡åŒ–æ¨¡å‹è¼¸å‡º...\\n\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡æ½”å›ç­”ã€‚\"),\n",
    "    HumanMessage(\"ä»€éº¼æ˜¯æ¨¡å‹é‡åŒ–ï¼Ÿè«‹ç”¨ 2-3 å¥è©±èªªæ˜ã€‚\")\n",
    "]\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "print(\"ğŸ¤– æ¨¡å‹å›ç­”ï¼š\")\n",
    "print(response.content)\n",
    "print(\"\\nâœ… é‡åŒ–æ¨¡å‹æ¸¬è©¦æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Output Parser åŸºç¤æ•™å­¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: StrOutputParser - åŸºæœ¬å­—ä¸²è§£æ\n",
    "print(\"ğŸ“ ç¯„ä¾‹ 1: StrOutputParser (å­—ä¸²è§£æå™¨)\\n\")\n",
    "\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"è«‹ç”¨ä¸€å¥è©±è§£é‡‹ï¼š{concept}\",\n",
    "    input_variables=[\"concept\"]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_model | str_parser\n",
    "\n",
    "result = chain.invoke({\"concept\": \"RAG (Retrieval-Augmented Generation)\"})\n",
    "print(f\"è¼¸å‡ºé¡å‹: {type(result)}\")\n",
    "print(f\"å…§å®¹: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: CommaSeparatedListOutputParser - åˆ—è¡¨è§£æ\n",
    "print(\"ğŸ“‹ ç¯„ä¾‹ 2: CommaSeparatedListOutputParser (åˆ—è¡¨è§£æå™¨)\\n\")\n",
    "\n",
    "list_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"åˆ—å‡º 5 å€‹{category}ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼Œåªè¼¸å‡ºé …ç›®åç¨±ï¼š\",\n",
    "    input_variables=[\"category\"]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_model | list_parser\n",
    "\n",
    "result = chain.invoke({\"category\": \"æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•\"})\n",
    "print(f\"è¼¸å‡ºé¡å‹: {type(result)}\")\n",
    "print(f\"åˆ—è¡¨é …ç›®:\")\n",
    "for i, item in enumerate(result, 1):\n",
    "    print(f\"  {i}. {item.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: JsonOutputParser - JSON æ ¼å¼è§£æï¼ˆæ”¹é€²ç‰ˆï¼‰\nprint(\"ğŸ”§ ç¯„ä¾‹ 3: JsonOutputParser (JSON è§£æå™¨)\\n\")\n\n# å®šç¾©ç°¡å–®çš„ JSON schema\nclass Product(BaseModel):\n    name: str = Field(description=\"ç”¢å“åç¨±\")\n    category: str = Field(description=\"ç”¢å“é¡åˆ¥\")\n    price: int = Field(description=\"åƒ¹æ ¼ï¼ˆæ–°å°å¹£ï¼‰\")\n    in_stock: bool = Field(description=\"æ˜¯å¦æœ‰åº«å­˜\")\n\njson_parser = JsonOutputParser(pydantic_object=Product)\n\n# æ”¹é€²çš„ Prompt - ä½¿ç”¨ Few-Shot ç¯„ä¾‹\nprompt = PromptTemplate(\n    template=\"\"\"å¾ç”¢å“æè¿°ä¸­æå–è³‡è¨Šä¸¦è¼¸å‡º JSON æ ¼å¼ã€‚\n\nç¯„ä¾‹ï¼š\nç”¢å“æè¿°: iPhone 15 æ˜¯ Apple çš„æ™ºæ…§æ‰‹æ©Ÿï¼Œå”®åƒ¹ 32900 å…ƒï¼Œç›®å‰ç¼ºè²¨\nJSON: {{\"name\": \"iPhone 15\", \"category\": \"æ™ºæ…§æ‰‹æ©Ÿ\", \"price\": 32900, \"in_stock\": false}}\n\nç¾åœ¨è«‹è™•ç†ï¼š\nç”¢å“æè¿°: {description}\nJSON:\"\"\",\n    input_variables=[\"description\"]\n)\n\n# å»ºç«‹ LLMï¼ˆç›´æ¥ç”¨åƒæ•¸ï¼Œä¸ä½¿ç”¨ GenerationConfig ç‰©ä»¶ï¼‰\njson_llm = HuggingFacePipeline.from_model_id(\n    model_id=\"google/gemma-3-1b-it\",\n    task=\"text-generation\",\n    model_kwargs={\n        \"quantization_config\": quantization_config,\n        \"device_map\": \"auto\"\n    },\n    pipeline_kwargs={\n        \"max_new_tokens\": 128,\n        \"do_sample\": False,        # ä¸ä½¿ç”¨æ¡æ¨£ï¼ˆç¢ºå®šæ€§è¼¸å‡ºï¼‰\n        \"pad_token_id\": 0,         # è¨­å®š padding token\n        \"eos_token_id\": 1          # è¨­å®š end-of-sequence token\n    }\n)\njson_chat = ChatHuggingFace(llm=json_llm)\n\nchain = prompt | json_chat | json_parser\n\ndescription = \"MacBook Pro æ˜¯ Apple çš„å°ˆæ¥­ç­†è¨˜å‹é›»è…¦ï¼Œå”®åƒ¹ 68900 å…ƒï¼Œç›®å‰æœ‰ç¾è²¨\"\n\nprint(f\"ğŸ“ ç”¢å“æè¿°: {description}\\n\")\n\ntry:\n    result = chain.invoke({\"description\": description})\n    \n    print(f\"âœ… è§£ææˆåŠŸï¼\")\n    print(f\"è¼¸å‡ºé¡å‹: {type(result)}\\n\")\n    print(f\"JSON å…§å®¹:\")\n    print(json.dumps(result, indent=2, ensure_ascii=False))\n    \n    # é©—è­‰è³‡æ–™å®Œæ•´æ€§\n    print(f\"\\nğŸ“¦ è³‡æ–™é©—è­‰:\")\n    required_fields = [\"name\", \"category\", \"price\", \"in_stock\"]\n    missing_fields = [f for f in required_fields if f not in result]\n    \n    if missing_fields:\n        print(f\"  âš ï¸  ç¼ºå°‘æ¬„ä½: {', '.join(missing_fields)}\")\n    else:\n        print(f\"  âœ… æ‰€æœ‰å¿…è¦æ¬„ä½éƒ½å­˜åœ¨\")\n        print(f\"\\nğŸ“‹ ç”¢å“è³‡è¨Š:\")\n        print(f\"  åç¨±: {result['name']}\")\n        print(f\"  é¡åˆ¥: {result['category']}\")\n        print(f\"  åƒ¹æ ¼: NT$ {result['price']:,}\")\n        print(f\"  åº«å­˜: {'æœ‰è²¨ âœ…' if result['in_stock'] else 'ç¼ºè²¨ âŒ'}\")\n    \nexcept Exception as e:\n    print(f\"âŒ è§£æéŒ¯èª¤: {e}\")\n    \n    # é™¤éŒ¯ï¼šé¡¯ç¤ºåŸå§‹ LLM è¼¸å‡º\n    print(f\"\\nğŸ” é™¤éŒ¯è³‡è¨Š - LLM åŸå§‹è¼¸å‡º:\")\n    try:\n        debug_chain = prompt | json_chat\n        debug_result = debug_chain.invoke({\"description\": description})\n        print(debug_result.content[:500])  # åªé¡¯ç¤ºå‰ 500 å­—å…ƒ\n        \n        # å˜—è©¦æ‰‹å‹•è§£æ\n        print(f\"\\nğŸ’¡ å˜—è©¦æ‰‹å‹•è§£æ JSON...\")\n        import re\n        json_match = re.search(r'\\{[^{}]+\\}', debug_result.content)\n        if json_match:\n            manual_json = json.loads(json_match.group())\n            print(f\"âœ… æ‰‹å‹•è§£ææˆåŠŸ:\")\n            print(json.dumps(manual_json, indent=2, ensure_ascii=False))\n        else:\n            print(\"âŒ æ‰¾ä¸åˆ° JSON æ ¼å¼\")\n        \n    except Exception as debug_e:\n        print(f\"é™¤éŒ¯å¤±æ•—: {debug_e}\")\n    \n    print(\"\\nğŸ’¡ èª¿æ•´å»ºè­°ï¼š\")\n    print(\"- å·²ä½¿ç”¨ç›´æ¥åƒæ•¸è¨­å®šç¢ºå®šæ€§è¼¸å‡º\")\n    print(\"- å¦‚æœæŒçºŒå¤±æ•—ï¼Œå»ºè­°æ”¹ç”¨ PydanticOutputParser (è¦‹ Cell 11)\")\n    print(\"- æˆ–ä½¿ç”¨æ›´ç°¡åŒ–çš„ prompt å’Œæ›´å°çš„æ¨¡å‹è¼¸å‡º\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 11: PydanticOutputParser - å‹åˆ¥å®‰å…¨çš„çµæ§‹åŒ–è¼¸å‡º\nprint(\"ğŸ¯ ç¯„ä¾‹ 4: PydanticOutputParser (Pydantic è§£æå™¨)\\n\")\n\n# å®šç¾©ç°¡åŒ–çš„è³‡æ–™æ¨¡å‹ï¼ˆæ¸›å°‘æ¬„ä½ä»¥æé«˜æˆåŠŸç‡ï¼‰\nclass TechArticleSummary(BaseModel):\n    title: str = Field(description=\"æ–‡ç« æ¨™é¡Œ\")\n    main_topic: str = Field(description=\"ä¸»è¦ä¸»é¡Œ\")\n    key_points: List[str] = Field(description=\"é—œéµé‡é»ï¼Œ2-3 é»\")\n    difficulty: str = Field(description=\"é›£åº¦ï¼šåˆç´š/ä¸­ç´š/é€²éš\")\n\npydantic_parser = PydanticOutputParser(pydantic_object=TechArticleSummary)\n\n# æ”¹é€² Prompt - ä½¿ç”¨ Few-Shot ç¯„ä¾‹ï¼Œä¸ä½¿ç”¨ format_instructions\nprompt = PromptTemplate(\n    template=\"\"\"åˆ†ææŠ€è¡“æ–‡ç« ä¸¦æå–çµæ§‹åŒ–è³‡è¨Šï¼Œä»¥ JSON æ ¼å¼è¼¸å‡ºã€‚\n\nç¯„ä¾‹ï¼š\næ–‡ç« ï¼šDocker æ˜¯ä¸€å€‹å®¹å™¨åŒ–å¹³å°ï¼Œå¯ä»¥æ‰“åŒ…æ‡‰ç”¨ç¨‹å¼åŠå…¶ä¾è³´ã€‚å®ƒç°¡åŒ–äº†éƒ¨ç½²æµç¨‹ï¼Œé©åˆå¾®æœå‹™æ¶æ§‹ã€‚é–‹ç™¼è€…å¯ä»¥ç¢ºä¿ç’°å¢ƒä¸€è‡´æ€§ã€‚\nJSON: {{\n  \"title\": \"Docker å®¹å™¨åŒ–æŠ€è¡“ä»‹ç´¹\",\n  \"main_topic\": \"å®¹å™¨åŒ–èˆ‡éƒ¨ç½²\",\n  \"key_points\": [\"æ‰“åŒ…æ‡‰ç”¨ç¨‹å¼åŠä¾è³´\", \"ç°¡åŒ–éƒ¨ç½²æµç¨‹\", \"ç¢ºä¿ç’°å¢ƒä¸€è‡´æ€§\"],\n  \"difficulty\": \"ä¸­ç´š\"\n}}\n\nç¾åœ¨è«‹åˆ†æï¼š\næ–‡ç« ï¼š{article}\nJSON:\"\"\",\n    input_variables=[\"article\"]\n)\n\n# å»ºç«‹ç©©å®šçš„ LLMï¼ˆä¸ä½¿ç”¨ GenerationConfig ç‰©ä»¶ï¼Œç›´æ¥ç”¨åƒæ•¸ï¼‰\nstable_llm = HuggingFacePipeline.from_model_id(\n    model_id=\"google/gemma-3-1b-it\",\n    task=\"text-generation\",\n    model_kwargs={\n        \"quantization_config\": quantization_config,\n        \"device_map\": \"auto\"\n    },\n    pipeline_kwargs={\n        \"max_new_tokens\": 256,\n        \"do_sample\": False,        # ç¢ºå®šæ€§è¼¸å‡º\n        \"pad_token_id\": 0,\n        \"eos_token_id\": 1\n    }\n)\nstable_chat = ChatHuggingFace(llm=stable_llm)\n\nchain = prompt | stable_chat | pydantic_parser\n\narticle = \"\"\"RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç¨®çµåˆæª¢ç´¢èˆ‡ç”Ÿæˆçš„ AI æŠ€è¡“ã€‚\nå®ƒå…ˆå¾çŸ¥è­˜åº«ä¸­æª¢ç´¢ç›¸é—œè³‡è¨Šï¼Œå†å°‡æª¢ç´¢çµæœèˆ‡ç”¨æˆ¶å•é¡Œä¸€èµ·å‚³çµ¦å¤§å‹èªè¨€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆã€‚\né€™ç¨®æ–¹æ³•å¯ä»¥æœ‰æ•ˆæ¸›å°‘ AI å¹»è¦ºå•é¡Œï¼Œæä¾›æ›´æº–ç¢ºä¸”æœ‰ä¾æ“šçš„å›ç­”ã€‚\"\"\"\n\nprint(f\"ğŸ“ æ–‡ç« å…§å®¹:\\n{article}\\n\")\n\ntry:\n    result = chain.invoke({\"article\": article})\n    print(f\"âœ… è§£ææˆåŠŸï¼é¡å‹: {type(result)}\\n\")\n    print(f\"ğŸ“Œ æ¨™é¡Œ: {result.title}\")\n    print(f\"ğŸ¯ ä¸»é¡Œ: {result.main_topic}\")\n    print(f\"ğŸ“Š é›£åº¦: {result.difficulty}\")\n    print(f\"\\nğŸ’¡ é—œéµé‡é»:\")\n    for i, point in enumerate(result.key_points, 1):\n        print(f\"  {i}. {point}\")\n        \nexcept Exception as e:\n    print(f\"âŒ è§£æéŒ¯èª¤: {str(e)[:200]}...\\n\")\n    \n    # é™¤éŒ¯ï¼šé¡¯ç¤ºåŸå§‹è¼¸å‡º\n    print(\"ğŸ” é™¤éŒ¯è³‡è¨Š - LLM åŸå§‹è¼¸å‡º:\")\n    try:\n        debug_chain = prompt | stable_chat\n        debug_result = debug_chain.invoke({\"article\": article})\n        print(debug_result.content[:500])\n        \n        # å˜—è©¦æ‰‹å‹•è§£æ\n        print(\"\\nğŸ’¡ å˜—è©¦æ‰‹å‹•è§£æ...\")\n        import re\n        json_match = re.search(r'\\{[^{}]*\\[[^\\]]*\\][^{}]*\\}|\\{[^{}]+\\}', debug_result.content, re.DOTALL)\n        if json_match:\n            try:\n                manual_json = json.loads(json_match.group())\n                print(\"âœ… æ‰¾åˆ° JSON çµæ§‹:\")\n                print(json.dumps(manual_json, indent=2, ensure_ascii=False))\n                \n                # å˜—è©¦ç”¨æ‰¾åˆ°çš„ JSON å»ºç«‹ç‰©ä»¶\n                manual_result = TechArticleSummary(**manual_json)\n                print(\"\\nâœ… æ‰‹å‹•å»ºç«‹ Pydantic ç‰©ä»¶æˆåŠŸï¼\")\n                print(f\"æ¨™é¡Œ: {manual_result.title}\")\n                print(f\"ä¸»é¡Œ: {manual_result.main_topic}\")\n                print(f\"é›£åº¦: {manual_result.difficulty}\")\n                print(f\"é‡é»æ•¸é‡: {len(manual_result.key_points)}\")\n                \n            except Exception as parse_e:\n                print(f\"âŒ æ‰‹å‹•è§£æä¹Ÿå¤±æ•—: {parse_e}\")\n        else:\n            print(\"âŒ æ‰¾ä¸åˆ°å®Œæ•´çš„ JSON çµæ§‹\")\n            \n    except Exception as debug_e:\n        print(f\"é™¤éŒ¯å¤±æ•—: {debug_e}\")\n    \n    print(\"\\nğŸ’¡ èª¿æ•´å»ºè­°ï¼š\")\n    print(\"- å·²ç°¡åŒ–æ¨¡å‹æ¬„ä½ï¼ˆç§»é™¤ target_audienceï¼‰\")\n    print(\"- å·²ä½¿ç”¨ Few-Shot ç¯„ä¾‹å¼•å°\")\n    print(\"- å·²è¨­å®š do_sample=False ç¢ºä¿ç©©å®šæ€§\")\n    print(\"- å¯ä»¥å˜—è©¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆgemma-3-4b-itï¼‰\")\n    print(\"- æˆ–æ”¹ç”¨æ›´ç°¡å–®çš„ JsonOutputParserï¼ˆè¦‹ Cell 10ï¼‰\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Output Parser æ¯”è¼ƒç¸½çµ\n",
    "print(\"\"\"ğŸ“Š Output Parser é¡å‹æ¯”è¼ƒ\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Parser é¡å‹             â”‚ è¼¸å‡ºé¡å‹         â”‚ é©ç”¨å ´æ™¯            â”‚ é›£åº¦        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ StrOutputParser         â”‚ str              â”‚ ç°¡å–®æ–‡å­—å›ç­”        â”‚ â­          â”‚\n",
    "â”‚ CommaSeparatedList      â”‚ List[str]        â”‚ åˆ—è¡¨é …ç›®            â”‚ â­â­        â”‚\n",
    "â”‚ JsonOutputParser        â”‚ Dict             â”‚ çµæ§‹åŒ–è³‡æ–™          â”‚ â­â­â­      â”‚\n",
    "â”‚ PydanticOutputParser    â”‚ Pydantic Model   â”‚ å‹åˆ¥å®‰å…¨è³‡æ–™        â”‚ â­â­â­â­    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ’¡ é¸æ“‡å»ºè­°ï¼š\n",
    "1. ç°¡å–®å›ç­” â†’ StrOutputParser\n",
    "2. éœ€è¦åˆ—è¡¨ â†’ CommaSeparatedListOutputParser\n",
    "3. éœ€è¦é©—è­‰ â†’ PydanticOutputParser (æ¨è–¦ï¼)\n",
    "4. å½ˆæ€§è³‡æ–™ â†’ JsonOutputParser\n",
    "\n",
    "âš ï¸  æ³¨æ„äº‹é …ï¼š\n",
    "- çµæ§‹åŒ–è¼¸å‡ºå»ºè­°ä½¿ç”¨ temperature=0 æˆ–æ¥µä½å€¼\n",
    "- Pydantic æä¾›è‡ªå‹•å‹åˆ¥æª¢æŸ¥å’Œé©—è­‰\n",
    "- è¤‡é›œçµæ§‹å¯èƒ½éœ€è¦å¤šæ¬¡é‡è©¦\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: RAG ç³»çµ±å»ºç«‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: è¼‰å…¥è«–æ–‡è³‡æ–™\n",
    "print(\"ğŸ“š è¼‰å…¥å­¸è¡“è«–æ–‡...\\n\")\n",
    "\n",
    "# ä½¿ç”¨ Multi-Agent Debate è«–æ–‡\n",
    "pdf_path = \"2509.05396v1.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"âœ… æˆåŠŸè¼‰å…¥è«–æ–‡\")\n",
    "print(f\"ğŸ“„ ç¸½é æ•¸: {len(documents)}\")\n",
    "print(f\"ğŸ“ ç¬¬ä¸€é å‰ 200 å­—å…ƒ:\\n{documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: æ–‡ä»¶åˆ†å‰²èˆ‡å‘é‡åŒ–\n",
    "print(\"âœ‚ï¸  æ–‡ä»¶åˆ†å‰²èˆ‡å‘é‡åŒ–...\\n\")\n",
    "\n",
    "# æ–‡ä»¶åˆ†å‰²\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"ğŸ“¦ åˆ†å‰²æˆ {len(texts)} å€‹æ–‡å­—å€å¡Š\")\n",
    "\n",
    "# å»ºç«‹ Embeddings\n",
    "print(\"\\nğŸ”„ å»ºç«‹å‘é‡åµŒå…¥...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# å»ºç«‹å‘é‡è³‡æ–™åº«\n",
    "print(\"ğŸ—„ï¸  å»ºç«‹ FAISS å‘é‡è³‡æ–™åº«...\")\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "print(\"\\nâœ… RAG è³‡æ–™æº–å‚™å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: å»ºç«‹åŸºç¤ RAG Chain (ç„¡ Parser)\n",
    "print(\"ğŸ”— å»ºç«‹åŸºç¤ RAG Chain\\n\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"æ ¹æ“šä»¥ä¸‹è³‡è¨Šå›ç­”å•é¡Œã€‚å¦‚æœè³‡è¨Šä¸­æ²’æœ‰ç›¸é—œå…§å®¹ï¼Œè«‹èªªä½ ä¸çŸ¥é“ã€‚\n",
    "\n",
    "ç›¸é—œè³‡è¨Š:\n",
    "{context}\n",
    "\n",
    "å•é¡Œ: {question}\n",
    "\n",
    "å›ç­”:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦åŸºç¤ RAG\n",
    "query = \"è«‹å• multi-agent debate çš„ä¸»è¦ç™¼ç¾æ˜¯ä»€éº¼ï¼Ÿ\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"â“ å•é¡Œ: {query}\\n\")\n",
    "print(f\"ğŸ’¬ å›ç­”: {result['result']}\\n\")\n",
    "print(f\"ğŸ“Œ ä½¿ç”¨äº† {len(result['source_documents'])} å€‹ä¾†æºç‰‡æ®µ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Part 4.5: é€²éšæª¢ç´¢æŠ€è¡“ - BM25ã€Hybrid Searchã€Rerank",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 15.5: æª¢ç´¢æ–¹æ³•æ•ˆæœæ¯”è¼ƒ\nprint(\"ğŸ“Š ç¯„ä¾‹ 5: æª¢ç´¢æ–¹æ³•å…¨é¢æ¯”è¼ƒ\\n\")\n\nimport time\n\ndef compare_retrieval_methods(query: str):\n    \"\"\"æ¯”è¼ƒä¸åŒæª¢ç´¢æ–¹æ³•çš„æ•ˆæœ\"\"\"\n    results = {}\n    \n    # 1. Vector æª¢ç´¢\n    start = time.time()\n    vector_docs = retriever.invoke(query)[:3]\n    vector_time = time.time() - start\n    results['Vector'] = {\n        'docs': vector_docs,\n        'time': vector_time,\n        'method': 'èªç¾©å‘é‡æª¢ç´¢'\n    }\n    \n    # 2. BM25 æª¢ç´¢\n    start = time.time()\n    bm25_docs = bm25_retriever.invoke(query)[:3]\n    bm25_time = time.time() - start\n    results['BM25'] = {\n        'docs': bm25_docs,\n        'time': bm25_time,\n        'method': 'é—œéµå­—æª¢ç´¢'\n    }\n    \n    # 3. Hybrid æª¢ç´¢\n    start = time.time()\n    hybrid_docs = hybrid_retriever.invoke(query)[:3]\n    hybrid_time = time.time() - start\n    results['Hybrid'] = {\n        'docs': hybrid_docs,\n        'time': hybrid_time,\n        'method': 'æ··åˆæª¢ç´¢'\n    }\n    \n    # 4. Hybrid + Rerank\n    start = time.time()\n    reranked_docs = hybrid_rerank_retriever(query, k=3, candidate_k=10)\n    rerank_time = time.time() - start\n    results['Hybrid+Rerank'] = {\n        'docs': reranked_docs,\n        'time': rerank_time,\n        'method': 'æ··åˆæª¢ç´¢ + é‡æ’åº'\n    }\n    \n    return results\n\n# æ¸¬è©¦ä¸åŒé¡å‹çš„æŸ¥è©¢\ntest_cases = [\n    {\n        'query': 'multi-agent debate accuracy performance',\n        'type': 'é—œéµå­—å¯†é›†å‹æŸ¥è©¢'\n    },\n    {\n        'query': 'How can we improve the reasoning ability of language models?',\n        'type': 'èªç¾©ç†è§£å‹æŸ¥è©¢'\n    }\n]\n\nfor case in test_cases:\n    query = case['query']\n    query_type = case['type']\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"ğŸ”¬ æ¸¬è©¦æ¡ˆä¾‹: {query_type}\")\n    print(f\"â“ æŸ¥è©¢: {query}\\n\")\n    \n    # åŸ·è¡Œæ¯”è¼ƒ\n    comparison = compare_retrieval_methods(query)\n    \n    # é¡¯ç¤ºçµæœ\n    print(\"ğŸ“Š æª¢ç´¢çµæœæ¯”è¼ƒï¼š\\n\")\n    \n    for method_name, data in comparison.items():\n        print(f\"{'â”€'*80}\")\n        print(f\"ğŸ”¹ æ–¹æ³•: {method_name} ({data['method']})\")\n        print(f\"â±ï¸  è€—æ™‚: {data['time']*1000:.2f} ms\")\n        print(f\"ğŸ“„ æª¢ç´¢çµæœ:\")\n        \n        for i, doc in enumerate(data['docs'], 1):\n            preview = doc.page_content[:100].replace('\\n', ' ')\n            print(f\"   {i}. {preview}...\")\n        print()\n\nprint(f\"{'='*80}\\n\")\n\n# ç¸½çµè¡¨æ ¼\nprint(\"ğŸ“ˆ æª¢ç´¢æ–¹æ³•ç‰¹æ€§ç¸½çµï¼š\\n\")\nprint(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\nprint(\"â”‚ æ–¹æ³•            â”‚ å„ªé»             â”‚ ç¼ºé»         â”‚ é©ç”¨å ´æ™¯        â”‚\")\nprint(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\nprint(\"â”‚ Vector          â”‚ èªç¾©ç†è§£å¼·       â”‚ é—œéµå­—å¼±     â”‚ èªç¾©æŸ¥è©¢        â”‚\")\nprint(\"â”‚ BM25            â”‚ é—œéµå­—ç²¾ç¢º       â”‚ ç„¡èªç¾©ç†è§£   â”‚ ç²¾ç¢ºåŒ¹é…        â”‚\")\nprint(\"â”‚ Hybrid          â”‚ å…¼å…·å…©è€…å„ªé»     â”‚ åƒæ•¸èª¿æ•´è¤‡é›œ â”‚ é€šç”¨æŸ¥è©¢        â”‚\")\nprint(\"â”‚ Hybrid+Rerank   â”‚ æœ€é«˜æº–ç¢ºåº¦       â”‚ é€Ÿåº¦è¼ƒæ…¢     â”‚ å“è³ªè¦æ±‚é«˜      â”‚\")\nprint(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n\nprint(\"\\nğŸ’¡ é¸æ“‡å»ºè­°ï¼š\")\nprint(\"â€¢ å»¶é²æ•æ„Ÿ â†’ Vector æˆ– BM25\")\nprint(\"â€¢ å“è³ªå„ªå…ˆ â†’ Hybrid + Rerank (æ¨è–¦ï¼)\")\nprint(\"â€¢ å¹³è¡¡æ–¹æ¡ˆ â†’ Hybrid\")\nprint(\"â€¢ ç‰¹å®šé ˜åŸŸ â†’ æ ¹æ“šæŸ¥è©¢é¡å‹é¸æ“‡\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 15.4: å®Œæ•´çš„ Hybrid + Rerank RAG Pipeline\nprint(\"ğŸš€ ç¯„ä¾‹ 4: å®Œæ•´ Hybrid + Rerank RAG ç³»çµ±\\n\")\n\nfrom langchain_core.runnables import RunnableLambda\n\nprint(\"ğŸ“˜ å®Œæ•´ Pipeline æµç¨‹ï¼š\")\nprint(\"1ï¸âƒ£  Hybrid æª¢ç´¢ï¼šBM25 + Vector æ··åˆæª¢ç´¢\")\nprint(\"2ï¸âƒ£  Rerankï¼šä½¿ç”¨ Cross-Encoder é‡æ–°æ’åº\")\nprint(\"3ï¸âƒ£  Context çµ„åˆï¼šå°‡æœ€ç›¸é—œæ–‡æª”çµ„åˆç‚ºä¸Šä¸‹æ–‡\")\nprint(\"4ï¸âƒ£  LLM ç”Ÿæˆï¼šåŸºæ–¼å„ªè³ªä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ\\n\")\n\ndef hybrid_rerank_retriever(query: str, k: int = 3, candidate_k: int = 10):\n    \"\"\"Hybrid + Rerank æª¢ç´¢å™¨\"\"\"\n    # Step 1: Hybrid æª¢ç´¢\n    candidates = hybrid_retriever.invoke(query)[:candidate_k]\n    \n    # Step 2: Rerank\n    reranked = rerank_documents(query, candidates, top_k=k)\n    \n    # è¿”å›æ–‡æª”ï¼ˆä¸å«åˆ†æ•¸ï¼‰\n    return [doc for doc, score in reranked]\n\n# å»ºç«‹å®Œæ•´çš„ RAG Chain\nadvanced_rag_prompt = PromptTemplate(\n    template=\"\"\"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å­¸è¡“è«–æ–‡åˆ†æåŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šè©³ç´°å›ç­”å•é¡Œã€‚\n\nç›¸é—œè³‡è¨Š:\n{context}\n\nå•é¡Œ: {question}\n\nè«‹æä¾›ï¼š\n1. ç›´æ¥å›ç­”å•é¡Œ\n2. å¼•ç”¨ç›¸é—œè­‰æ“š\n3. å¦‚æœè³‡è¨Šä¸è¶³ï¼Œè«‹èªªæ˜\n\nå›ç­”:\"\"\",\n    input_variables=[\"context\", \"question\"]\n)\n\n# æ¸¬è©¦å®Œæ•´ç³»çµ±\ntest_queries = [\n    \"What is the main contribution of this research?\",\n    \"How does multi-agent debate improve performance?\",\n    \"What are the limitations of this approach?\"\n]\n\nfor query in test_queries:\n    print(f\"\\n{'='*70}\")\n    print(f\"â“ å•é¡Œ: {query}\\n\")\n    \n    # ä½¿ç”¨ Hybrid + Rerank æª¢ç´¢\n    print(\"ğŸ” åŸ·è¡Œ Hybrid + Rerank æª¢ç´¢...\")\n    relevant_docs = hybrid_rerank_retriever(query, k=3, candidate_k=10)\n    \n    print(f\"âœ… æª¢ç´¢åˆ° {len(relevant_docs)} å€‹é«˜ç›¸é—œæ–‡æª”\\n\")\n    \n    # çµ„åˆä¸Šä¸‹æ–‡\n    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n    \n    # ç”Ÿæˆç­”æ¡ˆ\n    print(\"ğŸ¤– ç”Ÿæˆç­”æ¡ˆ...\\n\")\n    formatted_prompt = advanced_rag_prompt.format(context=context, question=query)\n    response = chat_model.invoke([HumanMessage(content=formatted_prompt)])\n    \n    print(\"ğŸ’¬ å›ç­”:\")\n    print(response.content)\n    print(f\"\\nğŸ“š ä½¿ç”¨ä¾†æº: {len(relevant_docs)} å€‹æ–‡æª”ç‰‡æ®µ\")\n    print(f\"ğŸ“„ ä¾†æºé ç¢¼: {', '.join([str(doc.metadata.get('page', 'N/A')) for doc in relevant_docs])}\")\n\nprint(f\"\\n{'='*70}\\n\")\nprint(\"âœ¨ ç³»çµ±å„ªå‹¢ç¸½çµï¼š\")\nprint(\"âœ… Hybrid Search - çµåˆé—œéµå­—èˆ‡èªç¾©æª¢ç´¢\")\nprint(\"âœ… Reranker - ç²¾ç¢ºæ’åºæœ€ç›¸é—œæ–‡æª”\")\nprint(\"âœ… é«˜å“è³ªä¸Šä¸‹æ–‡ - æå‡ LLM å›ç­”æº–ç¢ºåº¦\")\nprint(\"âœ… æ¸›å°‘å¹»è¦º - åŸºæ–¼çœŸå¯¦æ–‡æª”å…§å®¹ç”Ÿæˆ\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 15.3: Reranker - é‡æ–°æ’åºæå‡ç›¸é—œæ€§\nprint(\"ğŸ¯ ç¯„ä¾‹ 3: Reranker (é‡æ–°æ’åº)\\n\")\n\nfrom sentence_transformers import CrossEncoder\nimport numpy as np\n\nprint(\"ğŸ“˜ Reranker åŸç†ï¼š\")\nprint(\"- ä½¿ç”¨ Cross-Encoder æ¨¡å‹å°æª¢ç´¢çµæœé‡æ–°è©•åˆ†\")\nprint(\"- Cross-Encoder åŒæ™‚è€ƒæ…®æŸ¥è©¢å’Œæ–‡æª”ï¼Œè¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸\")\nprint(\"- æ¯” Bi-Encoder (å‘é‡æª¢ç´¢) æ›´ç²¾ç¢ºï¼Œä½†é€Ÿåº¦è¼ƒæ…¢\")\nprint(\"- é©åˆå°åˆæ­¥æª¢ç´¢çµæœé€²è¡Œç²¾ç…‰\\n\")\n\n# è¼‰å…¥ Reranker æ¨¡å‹ï¼ˆä½¿ç”¨è¼•é‡ç´šæ¨¡å‹ä»¥ç¯€çœè³‡æºï¼‰\nprint(\"ğŸ”„ è¼‰å…¥ Reranker æ¨¡å‹...\")\nreranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\nprint(\"âœ… Reranker æ¨¡å‹è¼‰å…¥å®Œæˆ\\n\")\n\ndef rerank_documents(query: str, documents: list, top_k: int = 3):\n    \"\"\"ä½¿ç”¨ Reranker å°æ–‡æª”é‡æ–°æ’åº\"\"\"\n    # æº–å‚™ query-document pairs\n    pairs = [[query, doc.page_content] for doc in documents]\n    \n    # è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸\n    scores = reranker.predict(pairs)\n    \n    # æŒ‰åˆ†æ•¸æ’åº\n    sorted_indices = np.argsort(scores)[::-1][:top_k]\n    \n    # è¿”å›æ’åºå¾Œçš„æ–‡æª”å’Œåˆ†æ•¸\n    reranked_docs = [(documents[i], scores[i]) for i in sorted_indices]\n    return reranked_docs\n\n# æ¸¬è©¦ Reranker\nquery = \"What are the key findings about multi-agent debate?\"\n\nprint(f\"â“ æŸ¥è©¢: {query}\\n\")\n\n# å…ˆç”¨ Hybrid æª¢ç´¢ç²å–å€™é¸æ–‡æª”\nprint(\"ğŸ”€ æ­¥é©Ÿ 1: Hybrid æª¢ç´¢ (ç²å– 10 å€‹å€™é¸)\")\ncandidates = hybrid_retriever.invoke(query)[:10]\nprint(f\"   å–å¾— {len(candidates)} å€‹å€™é¸æ–‡æª”\\n\")\n\n# ä½¿ç”¨ Reranker é‡æ–°æ’åº\nprint(\"ğŸ¯ æ­¥é©Ÿ 2: Reranker é‡æ–°æ’åº\")\nreranked_results = rerank_documents(query, candidates, top_k=3)\n\nprint(\"\\nğŸ“Š Reranked çµæœ (æŒ‰ç›¸é—œæ€§æ’åº):\\n\")\nfor i, (doc, score) in enumerate(reranked_results, 1):\n    print(f\"æ’å {i} (åˆ†æ•¸: {score:.4f}):\")\n    print(f\"  å…§å®¹: {doc.page_content[:120]}...\")\n    print(f\"  ä¾†æº: ç¬¬ {doc.metadata.get('page', 'N/A')} é \")\n    print()\n\nprint(\"ğŸ’¡ Reranker å„ªå‹¢ï¼š\")\nprint(\"- æ›´ç²¾ç¢ºçš„ç›¸é—œæ€§è©•ä¼°\")\nprint(\"- æ¸›å°‘ä¸ç›¸é—œçµæœ\")\nprint(\"- æå‡ RAG ç³»çµ±æ•´é«”å“è³ª\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 15.2: Hybrid Search - çµåˆ BM25 èˆ‡å‘é‡æª¢ç´¢\nprint(\"ğŸ”€ ç¯„ä¾‹ 2: Hybrid Search (æ··åˆæª¢ç´¢)\\n\")\n\nfrom langchain.retrievers import EnsembleRetriever\n\nprint(\"ğŸ“˜ Hybrid Search åŸç†ï¼š\")\nprint(\"- çµåˆ BM25 (é—œéµå­—) å’Œ Vector (èªç¾©) æª¢ç´¢\")\nprint(\"- ä½¿ç”¨åŠ æ¬Šå¹³å‡åˆä½µå…©ç¨®æª¢ç´¢çµæœ\")\nprint(\"- å„ªé»ï¼šåŒæ™‚ç²å¾—ç²¾ç¢ºåŒ¹é…å’Œèªç¾©ç†è§£\\n\")\n\n# å»ºç«‹ Ensemble Retriever (æ··åˆæª¢ç´¢å™¨)\nhybrid_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, retriever],  # BM25 + Vector\n    weights=[0.5, 0.5]  # å„ä½” 50% æ¬Šé‡\n)\n\n# æ¯”è¼ƒæ¸¬è©¦ï¼šä½¿ç”¨æ›´èªç¾©åŒ–çš„æŸ¥è©¢\ntest_queries = [\n    \"What is the main contribution of this paper?\",  # èªç¾©æŸ¥è©¢\n    \"multi-agent debate performance results\",  # é—œéµå­—æŸ¥è©¢\n]\n\nfor query in test_queries:\n    print(f\"\\n{'='*60}\")\n    print(f\"â“ æŸ¥è©¢: {query}\\n\")\n    \n    # Vector æª¢ç´¢\n    print(\"ğŸ¯ Vector æª¢ç´¢ (å‰ 2 å€‹):\")\n    vector_results = retriever.invoke(query)[:2]\n    for i, doc in enumerate(vector_results, 1):\n        print(f\"  {i}. {doc.page_content[:80]}... (é  {doc.metadata.get('page', 'N/A')})\")\n    \n    # BM25 æª¢ç´¢\n    print(\"\\nğŸ” BM25 æª¢ç´¢ (å‰ 2 å€‹):\")\n    bm25_results = bm25_retriever.invoke(query)[:2]\n    for i, doc in enumerate(bm25_results, 1):\n        print(f\"  {i}. {doc.page_content[:80]}... (é  {doc.metadata.get('page', 'N/A')})\")\n    \n    # Hybrid æª¢ç´¢\n    print(\"\\nğŸ”€ Hybrid æª¢ç´¢ (å‰ 3 å€‹):\")\n    hybrid_results = hybrid_retriever.invoke(query)[:3]\n    for i, doc in enumerate(hybrid_results, 1):\n        print(f\"  {i}. {doc.page_content[:80]}... (é  {doc.metadata.get('page', 'N/A')})\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"\\nğŸ’¡ è§€å¯Ÿé‡é»ï¼š\")\nprint(\"- èªç¾©æŸ¥è©¢ï¼šVector è¡¨ç¾æ›´å¥½ï¼Œç†è§£å•é¡Œæ„åœ–\")\nprint(\"- é—œéµå­—æŸ¥è©¢ï¼šBM25 è¡¨ç¾æ›´å¥½ï¼Œç²¾ç¢ºåŒ¹é…\")\nprint(\"- Hybrid çµåˆå…©è€…å„ªå‹¢ï¼Œæ›´ç©©å¥\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 15.1: BM25 Retriever - é—œéµå­—æª¢ç´¢\nprint(\"ğŸ” ç¯„ä¾‹ 1: BM25 Retriever (é—œéµå­—æª¢ç´¢)\\n\")\n\nfrom langchain.retrievers import BM25Retriever\n\nprint(\"ğŸ“˜ BM25 æ¼”ç®—æ³•èªªæ˜ï¼š\")\nprint(\"- BM25 (Best Matching 25) æ˜¯ä¸€ç¨®åŸºæ–¼ TF-IDF çš„æ’åºå‡½æ•¸\")\nprint(\"- å„ªé»ï¼šé—œéµå­—åŒ¹é…ç²¾ç¢ºã€ä¸éœ€è¦å‘é‡åŒ–ã€é€Ÿåº¦å¿«\")\nprint(\"- ç¼ºé»ï¼šç„¡æ³•ç†è§£èªç¾©ã€å°åŒç¾©è©ä¸æ•æ„Ÿ\\n\")\n\n# å»ºç«‹ BM25 Retriever\nbm25_retriever = BM25Retriever.from_documents(texts)\nbm25_retriever.k = 3  # è¿”å›å‰ 3 å€‹çµæœ\n\n# æ¸¬è©¦æŸ¥è©¢\nquery = \"multi-agent debate methodology\"\n\nprint(f\"â“ æŸ¥è©¢: {query}\\n\")\nprint(\"ğŸ” BM25 æª¢ç´¢çµæœ:\\n\")\n\nbm25_results = bm25_retriever.invoke(query)\n\nfor i, doc in enumerate(bm25_results, 1):\n    print(f\"ğŸ“„ çµæœ {i}:\")\n    print(f\"å…§å®¹: {doc.page_content[:150]}...\")\n    print(f\"ä¾†æº: ç¬¬ {doc.metadata.get('page', 'N/A')} é \")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: RAG + Output Parser æ•´åˆæ‡‰ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: æ¡ˆä¾‹ 1 - è«–æ–‡é—œéµè³‡è¨Šæå–\n",
    "print(\"ğŸ“„ æ¡ˆä¾‹ 1: å­¸è¡“è«–æ–‡é—œéµè³‡è¨Šæå–\\n\")\n",
    "\n",
    "class PaperAnalysis(BaseModel):\n",
    "    title: str = Field(description=\"è«–æ–‡æ¨™é¡Œ\")\n",
    "    main_finding: str = Field(description=\"ä¸»è¦ç ”ç©¶ç™¼ç¾ï¼Œ1-2 å¥è©±\")\n",
    "    methodology: str = Field(description=\"ç ”ç©¶æ–¹æ³•ï¼Œç°¡è¦èªªæ˜\")\n",
    "    key_contributions: List[str] = Field(description=\"ä¸»è¦è²¢ç»ï¼Œ2-3 é»\")\n",
    "    confidence: float = Field(description=\"å›ç­”ä¿¡å¿ƒåº¦ 0-1\", ge=0, le=1)\n",
    "\n",
    "# å»ºç«‹ Parser\n",
    "paper_parser = PydanticOutputParser(pydantic_object=PaperAnalysis)\n",
    "\n",
    "# å»ºç«‹çµæ§‹åŒ– RAG Prompt\n",
    "paper_prompt = PromptTemplate(\n",
    "    template=\"\"\"æ ¹æ“šä»¥ä¸‹è«–æ–‡ç‰‡æ®µï¼Œæå–çµæ§‹åŒ–è³‡è¨Šã€‚\n",
    "\n",
    "è«–æ–‡ç‰‡æ®µ:\n",
    "{context}\n",
    "\n",
    "å•é¡Œ: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "åˆ†æçµæœ:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": paper_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# æ‰‹å‹•å¯¦ä½œ RAG + Parser æµç¨‹\n",
    "query = \"åˆ†æé€™ç¯‡è«–æ–‡çš„æ ¸å¿ƒå…§å®¹\"\n",
    "\n",
    "# 1. æª¢ç´¢ç›¸é—œæ–‡ä»¶\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs[:3]])\n",
    "\n",
    "# 2. ç”Ÿæˆçµæ§‹åŒ– prompt\n",
    "formatted_prompt = paper_prompt.format(context=context, question=query)\n",
    "\n",
    "# 3. å‘¼å« LLM\n",
    "response = stable_chat.invoke([HumanMessage(content=formatted_prompt)])\n",
    "\n",
    "# 4. è§£æè¼¸å‡º\n",
    "try:\n",
    "    analysis = paper_parser.parse(response.content)\n",
    "    \n",
    "    print(\"âœ… è«–æ–‡åˆ†æçµæœ:\\n\")\n",
    "    print(f\"ğŸ“Œ æ¨™é¡Œ: {analysis.title}\")\n",
    "    print(f\"\\nğŸ” ä¸»è¦ç™¼ç¾:\\n{analysis.main_finding}\")\n",
    "    print(f\"\\nğŸ› ï¸  ç ”ç©¶æ–¹æ³•:\\n{analysis.methodology}\")\n",
    "    print(f\"\\nğŸ’¡ ä¸»è¦è²¢ç»:\")\n",
    "    for i, contrib in enumerate(analysis.key_contributions, 1):\n",
    "        print(f\"  {i}. {contrib}\")\n",
    "    print(f\"\\nğŸ“Š ä¿¡å¿ƒåº¦: {analysis.confidence:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è§£æå¤±æ•—: {e}\")\n",
    "    print(f\"\\nåŸå§‹è¼¸å‡º:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: æ¡ˆä¾‹ 2 - æŠ€è¡“å•ç­”ç³»çµ±\n",
    "print(\"ğŸ’» æ¡ˆä¾‹ 2: çµæ§‹åŒ–æŠ€è¡“å•ç­”\\n\")\n",
    "\n",
    "class TechnicalQA(BaseModel):\n",
    "    question_type: str = Field(description=\"å•é¡Œé¡å‹ï¼šæ¦‚å¿µ/æ–¹æ³•/æ¯”è¼ƒ/å¯¦ä½œ\")\n",
    "    answer: str = Field(description=\"è©³ç´°å›ç­”\")\n",
    "    key_terms: List[str] = Field(description=\"é—œéµè¡“èªï¼Œ2-4 å€‹\")\n",
    "    difficulty: str = Field(description=\"é›£åº¦ï¼šåˆç´š/ä¸­ç´š/é€²éš\")\n",
    "    related_topics: List[str] = Field(description=\"ç›¸é—œä¸»é¡Œ\")\n",
    "    sources_used: int = Field(description=\"ä½¿ç”¨çš„ä¾†æºæ•¸é‡\")\n",
    "\n",
    "tech_parser = PydanticOutputParser(pydantic_object=TechnicalQA)\n",
    "\n",
    "tech_prompt = PromptTemplate(\n",
    "    template=\"\"\"ä½ æ˜¯ä¸€å€‹æŠ€è¡“å°ˆå®¶ã€‚æ ¹æ“šä»¥ä¸‹è³‡è¨Šå›ç­”å•é¡Œã€‚\n",
    "\n",
    "ç›¸é—œè³‡è¨Š:\n",
    "{context}\n",
    "\n",
    "æŠ€è¡“å•é¡Œ: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "çµæ§‹åŒ–å›ç­”:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": tech_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "query = \"multi-agent debate å’Œå‚³çµ±çš„å–®ä¸€ agent æœ‰ä»€éº¼å·®ç•°ï¼Ÿ\"\n",
    "\n",
    "# RAG + Parser æµç¨‹\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs[:3]])\n",
    "formatted_prompt = tech_prompt.format(context=context, question=query)\n",
    "response = stable_chat.invoke([HumanMessage(content=formatted_prompt)])\n",
    "\n",
    "try:\n",
    "    qa_result = tech_parser.parse(response.content)\n",
    "    \n",
    "    print(\"âœ… æŠ€è¡“å•ç­”çµæœ:\\n\")\n",
    "    print(f\"â“ å•é¡Œé¡å‹: {qa_result.question_type}\")\n",
    "    print(f\"ğŸ“Š é›£åº¦: {qa_result.difficulty}\")\n",
    "    print(f\"\\nğŸ’¬ å›ç­”:\\n{qa_result.answer}\")\n",
    "    print(f\"\\nğŸ”‘ é—œéµè¡“èª: {', '.join(qa_result.key_terms)}\")\n",
    "    print(f\"\\nğŸ”— ç›¸é—œä¸»é¡Œ:\")\n",
    "    for topic in qa_result.related_topics:\n",
    "        print(f\"  - {topic}\")\n",
    "    print(f\"\\nğŸ“š ä½¿ç”¨ä¾†æº: {qa_result.sources_used} å€‹ç‰‡æ®µ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è§£æå¤±æ•—: {e}\")\n",
    "    print(f\"\\nåŸå§‹è¼¸å‡º:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: æ¡ˆä¾‹ 3 - æ–¹æ³•æ¯”è¼ƒåˆ†æ\n",
    "print(\"âš–ï¸  æ¡ˆä¾‹ 3: ç ”ç©¶æ–¹æ³•æ¯”è¼ƒåˆ†æ\\n\")\n",
    "\n",
    "class MethodComparison(BaseModel):\n",
    "    method_name: str = Field(description=\"æ–¹æ³•åç¨±\")\n",
    "    advantages: List[str] = Field(description=\"å„ªé»ï¼Œ2-3 é»\")\n",
    "    disadvantages: List[str] = Field(description=\"ç¼ºé»ï¼Œ2-3 é»\")\n",
    "    use_cases: List[str] = Field(description=\"é©ç”¨å ´æ™¯\")\n",
    "    performance_note: str = Field(description=\"æ•ˆèƒ½èªªæ˜\")\n",
    "    recommendation: str = Field(description=\"ä½¿ç”¨å»ºè­°\")\n",
    "\n",
    "comparison_parser = PydanticOutputParser(pydantic_object=MethodComparison)\n",
    "\n",
    "comparison_prompt = PromptTemplate(\n",
    "    template=\"\"\"æ ¹æ“šä»¥ä¸‹è³‡è¨Šï¼Œåˆ†æç ”ç©¶æ–¹æ³•çš„å„ªç¼ºé»ã€‚\n",
    "\n",
    "ç ”ç©¶å…§å®¹:\n",
    "{context}\n",
    "\n",
    "åˆ†æå•é¡Œ: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "æ¯”è¼ƒåˆ†æ:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": comparison_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "query = \"åˆ†æ multi-agent debate æ–¹æ³•çš„å„ªç¼ºé»\"\n",
    "\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs[:4]])  # ä½¿ç”¨æ›´å¤šç‰‡æ®µ\n",
    "formatted_prompt = comparison_prompt.format(context=context, question=query)\n",
    "response = stable_chat.invoke([HumanMessage(content=formatted_prompt)])\n",
    "\n",
    "try:\n",
    "    comparison = comparison_parser.parse(response.content)\n",
    "    \n",
    "    print(\"âœ… æ–¹æ³•æ¯”è¼ƒåˆ†æ:\\n\")\n",
    "    print(f\"ğŸ“Œ æ–¹æ³•: {comparison.method_name}\\n\")\n",
    "    \n",
    "    print(\"âœ… å„ªé»:\")\n",
    "    for i, adv in enumerate(comparison.advantages, 1):\n",
    "        print(f\"  {i}. {adv}\")\n",
    "    \n",
    "    print(\"\\nâŒ ç¼ºé»:\")\n",
    "    for i, dis in enumerate(comparison.disadvantages, 1):\n",
    "        print(f\"  {i}. {dis}\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ é©ç”¨å ´æ™¯:\")\n",
    "    for use_case in comparison.use_cases:\n",
    "        print(f\"  â€¢ {use_case}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š æ•ˆèƒ½èªªæ˜:\\n{comparison.performance_note}\")\n",
    "    print(f\"\\nğŸ’¡ ä½¿ç”¨å»ºè­°:\\n{comparison.recommendation}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è§£æå¤±æ•—: {e}\")\n",
    "    print(f\"\\nåŸå§‹è¼¸å‡º:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: å•†æ¥­æ‡‰ç”¨å¯¦æˆ°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: å•†æ¥­æ‡‰ç”¨ - æ™ºæ…§å®¢æœç³»çµ±\n",
    "print(\"ğŸ¤ å•†æ¥­æ‡‰ç”¨: æ™ºæ…§å®¢æœç³»çµ±\\n\")\n",
    "\n",
    "class CustomerServiceResponse(BaseModel):\n",
    "    intent: str = Field(description=\"å®¢æˆ¶æ„åœ–ï¼šæŠ€è¡“æŸ¥è©¢/ç”¢å“æ¯”è¼ƒ/æ•…éšœæ’é™¤/å…¶ä»–\")\n",
    "    answer: str = Field(description=\"å®¢æœå›ç­”ï¼Œå‹å–„ä¸”å°ˆæ¥­\")\n",
    "    sentiment: str = Field(description=\"å•é¡Œæƒ…ç·’ï¼šæ­£é¢/ä¸­æ€§/æ€¥è¿«\")\n",
    "    confidence: float = Field(description=\"å›ç­”ä¿¡å¿ƒåº¦ 0-1\", ge=0, le=1)\n",
    "    suggested_action: str = Field(description=\"å»ºè­°å¾ŒçºŒå‹•ä½œ\")\n",
    "    escalate: bool = Field(description=\"æ˜¯å¦éœ€è¦è½‰äººå·¥\")\n",
    "    related_docs: List[str] = Field(description=\"ç›¸é—œæ–‡ä»¶æˆ–è³‡æº\")\n",
    "\n",
    "service_parser = PydanticOutputParser(pydantic_object=CustomerServiceResponse)\n",
    "\n",
    "service_prompt = PromptTemplate(\n",
    "    template=\"\"\"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI å®¢æœåŠ©æ‰‹ã€‚æ ¹æ“šçŸ¥è­˜åº«å›ç­”å®¢æˆ¶å•é¡Œã€‚\n",
    "\n",
    "çŸ¥è­˜åº«å…§å®¹:\n",
    "{context}\n",
    "\n",
    "å®¢æˆ¶å•é¡Œ: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "å®¢æœå›æ‡‰:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": service_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# æ¨¡æ“¬å®¢æˆ¶å•é¡Œ\n",
    "customer_query = \"æˆ‘æƒ³äº†è§£ multi-agent debate æ˜¯å¦é©åˆç”¨åœ¨æˆ‘å€‘çš„å•ç­”ç³»çµ±ï¼Ÿæœ‰ä»€éº¼é™åˆ¶å—ï¼Ÿ\"\n",
    "\n",
    "relevant_docs = retriever.get_relevant_documents(customer_query)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs[:3]])\n",
    "formatted_prompt = service_prompt.format(context=context, question=customer_query)\n",
    "response = stable_chat.invoke([HumanMessage(content=formatted_prompt)])\n",
    "\n",
    "try:\n",
    "    service_response = service_parser.parse(response.content)\n",
    "    \n",
    "    print(\"ğŸ’¼ å®¢æœç³»çµ±å›æ‡‰:\\n\")\n",
    "    print(f\"â“ å®¢æˆ¶å•é¡Œ: {customer_query}\\n\")\n",
    "    print(f\"ğŸ¯ æ„åœ–è­˜åˆ¥: {service_response.intent}\")\n",
    "    print(f\"ğŸ˜Š æƒ…ç·’åˆ†æ: {service_response.sentiment}\")\n",
    "    print(f\"ğŸ“Š ä¿¡å¿ƒåº¦: {service_response.confidence:.0%}\\n\")\n",
    "    print(f\"ğŸ’¬ å®¢æœå›ç­”:\\n{service_response.answer}\\n\")\n",
    "    print(f\"ğŸ‘‰ å»ºè­°å‹•ä½œ: {service_response.suggested_action}\")\n",
    "    print(f\"ğŸš¨ è½‰äººå·¥: {'æ˜¯' if service_response.escalate else 'å¦'}\\n\")\n",
    "    print(f\"ğŸ“š ç›¸é—œè³‡æº:\")\n",
    "    for doc in service_response.related_docs:\n",
    "        print(f\"  â€¢ {doc}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è§£æå¤±æ•—: {e}\")\n",
    "    print(f\"\\nåŸå§‹è¼¸å‡º:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶\n",
    "print(\"ğŸ”„ å¯¦ä½œéŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶\\n\")\n",
    "\n",
    "def safe_rag_with_parser(query: str, parser, prompt_template, max_retries=2):\n",
    "    \"\"\"å®‰å…¨çš„ RAG + Parser åŸ·è¡Œï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # 1. æª¢ç´¢\n",
    "            relevant_docs = retriever.get_relevant_documents(query)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs[:3]])\n",
    "            \n",
    "            # 2. ç”Ÿæˆ prompt\n",
    "            formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "            \n",
    "            # 3. å‘¼å« LLM\n",
    "            response = stable_chat.invoke([HumanMessage(content=formatted_prompt)])\n",
    "            \n",
    "            # 4. è§£æ\n",
    "            parsed_result = parser.parse(response.content)\n",
    "            \n",
    "            print(f\"âœ… ç¬¬ {attempt + 1} æ¬¡å˜—è©¦æˆåŠŸ\")\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"result\": parsed_result,\n",
    "                \"raw_response\": response.content,\n",
    "                \"attempts\": attempt + 1\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—: {str(e)[:100]}\")\n",
    "            \n",
    "            if attempt == max_retries - 1:\n",
    "                # æœ€å¾Œä¸€æ¬¡å˜—è©¦å¤±æ•—ï¼Œè¿”å›åŸå§‹è¼¸å‡º\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e),\n",
    "                    \"raw_response\": response.content if 'response' in locals() else None,\n",
    "                    \"attempts\": attempt + 1\n",
    "                }\n",
    "\n",
    "# æ¸¬è©¦\n",
    "test_query = \"multi-agent debate çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€éº¼ï¼Ÿ\"\n",
    "result = safe_rag_with_parser(\n",
    "    query=test_query,\n",
    "    parser=paper_parser,\n",
    "    prompt_template=paper_prompt\n",
    ")\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(f\"\\nğŸ‰ è§£ææˆåŠŸï¼ˆå…± {result['attempts']} æ¬¡å˜—è©¦ï¼‰\")\n",
    "    print(f\"çµæœé¡å‹: {type(result['result'])}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ è§£æå¤±æ•—ï¼ˆå…± {result['attempts']} æ¬¡å˜—è©¦ï¼‰\")\n",
    "    print(f\"éŒ¯èª¤: {result['error']}\")\n",
    "    if result['raw_response']:\n",
    "        print(f\"\\nåŸå§‹è¼¸å‡º:\\n{result['raw_response'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: é€²éšæŠ€å·§èˆ‡å„ªåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: è‡ªå®šç¾© Output Parser\n",
    "print(\"ğŸ› ï¸  å»ºç«‹è‡ªå®šç¾© Output Parser\\n\")\n",
    "\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from typing import Any\n",
    "\n",
    "class KeyValueParser(BaseOutputParser[Dict[str, str]]):\n",
    "    \"\"\"è§£æ Key: Value æ ¼å¼çš„è¼¸å‡º\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"è§£æ Key: Value æ ¼å¼æ–‡å­—\"\"\"\n",
    "        result = {}\n",
    "        lines = text.strip().split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) == 2:\n",
    "                    key = parts[0].strip()\n",
    "                    value = parts[1].strip()\n",
    "                    result[key] = value\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_format_instructions(self) -> str:\n",
    "        return \"\"\"è«‹ä½¿ç”¨ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼š\n",
    "Key1: Value1\n",
    "Key2: Value2\n",
    "Key3: Value3\"\"\"\n",
    "\n",
    "# æ¸¬è©¦è‡ªå®šç¾© Parser\n",
    "kv_parser = KeyValueParser()\n",
    "\n",
    "kv_prompt = PromptTemplate(\n",
    "    template=\"\"\"æå–è«–æ–‡çš„åŸºæœ¬è³‡è¨Šã€‚\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "è«–æ–‡å…§å®¹:\n",
    "{context}\n",
    "\n",
    "åŸºæœ¬è³‡è¨Š:\"\"\",\n",
    "    input_variables=[\"context\"],\n",
    "    partial_variables={\"format_instructions\": kv_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "relevant_docs = retriever.get_relevant_documents(\"è«–æ–‡è³‡è¨Š\")\n",
    "context = relevant_docs[0].page_content[:500]\n",
    "\n",
    "formatted_prompt = kv_prompt.format(context=context)\n",
    "response = stable_chat.invoke([HumanMessage(content=formatted_prompt)])\n",
    "\n",
    "try:\n",
    "    parsed = kv_parser.parse(response.content)\n",
    "    print(\"âœ… è‡ªå®šç¾© Parser è§£ææˆåŠŸ:\\n\")\n",
    "    for key, value in parsed.items():\n",
    "        print(f\"{key}: {value}\")\nexcept Exception as e:\n",
    "    print(f\"âŒ è§£æå¤±æ•—: {e}\")\n",
    "    print(f\"\\nåŸå§‹è¼¸å‡º:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: æ•ˆèƒ½å„ªåŒ–å»ºè­°\n",
    "print(\"\"\"ğŸš€ æ•ˆèƒ½å„ªåŒ–å»ºè­°\n",
    "\n",
    "## 1. æ¨¡å‹é‡åŒ–å„ªåŒ–\n",
    "âœ… å·²ä½¿ç”¨ 4-bit é‡åŒ–\n",
    "ğŸ“‰ è¨˜æ†¶é«”æ¸›å°‘ ~75%\n",
    "ğŸ”§ å¯å˜—è©¦:\n",
    "   - 8-bit é‡åŒ–ï¼ˆæ›´ç©©å®šä½†è¼ƒå¤§ï¼‰\n",
    "   - èª¿æ•´ compute_dtype (bfloat16 vs float16)\n",
    "\n",
    "## 2. Output Parser å„ªåŒ–\n",
    "ğŸ¯ Temperature è¨­å®š:\n",
    "   - çµæ§‹åŒ–è¼¸å‡º: 0.0 - 0.2\n",
    "   - å‰µæ„å›ç­”: 0.7 - 0.9\n",
    "   \n",
    "ğŸ”„ é‡è©¦ç­–ç•¥:\n",
    "   - å¯¦ä½œ max_retries (2-3 æ¬¡)\n",
    "   - ä½¿ç”¨ try-except æ•æ‰éŒ¯èª¤\n",
    "   - æä¾›é™ç´šæ–¹æ¡ˆ\n",
    "\n",
    "## 3. RAG ç³»çµ±å„ªåŒ–\n",
    "ğŸ“Š æª¢ç´¢åƒæ•¸:\n",
    "   - chunk_size: 300-1000 (ä¾æ–‡ä»¶é¡å‹)\n",
    "   - chunk_overlap: 50-200\n",
    "   - kå€¼: 3-10 (å¹³è¡¡å“è³ªèˆ‡é€Ÿåº¦)\n",
    "\n",
    "ğŸ” æª¢ç´¢ç­–ç•¥:\n",
    "   - æ··åˆæœå°‹ (é—œéµå­— + èªç¾©)\n",
    "   - Re-ranking (é‡æ–°æ’åº)\n",
    "   - Query expansion (æŸ¥è©¢æ“´å±•)\n",
    "\n",
    "## 4. Prompt Engineering\n",
    "ğŸ“ Prompt è¨­è¨ˆ:\n",
    "   - æ˜ç¢ºçš„æ ¼å¼æŒ‡ç¤º\n",
    "   - æä¾›ç¯„ä¾‹ï¼ˆFew-shotï¼‰\n",
    "   - å¼·èª¿è¼¸å‡ºæ ¼å¼è¦æ±‚\n",
    "   \n",
    "## 5. å¿«å–æ©Ÿåˆ¶\n",
    "ğŸ’¾ å»ºè­°å¯¦ä½œ:\n",
    "   - Embedding å¿«å–\n",
    "   - å¸¸è¦‹å•é¡Œå¿«å–\n",
    "   - å‘é‡è³‡æ–™åº«æŒä¹…åŒ–\n",
    "\n",
    "## 6. éŒ¯èª¤è™•ç†\n",
    "ğŸ›¡ï¸  é˜²ç¦¦ç­–ç•¥:\n",
    "   - é©—è­‰ Pydantic æ¬„ä½\n",
    "   - è¨­å®šé è¨­å€¼\n",
    "   - è¨˜éŒ„å¤±æ•—æ¡ˆä¾‹\n",
    "   - ç›£æ§æˆåŠŸç‡\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¸½çµèˆ‡ä½œæ¥­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: èª²ç¨‹ç¸½çµ\n",
    "print(\"\"\"ğŸ“š Week 6 èª²ç¨‹ç¸½çµ\n",
    "\n",
    "## ğŸ¯ æœ¬é€±å­¸ç¿’é‡é»\n",
    "\n",
    "### 1. é‡åŒ–æŠ€è¡“\n",
    "âœ… ç†è§£æ¨¡å‹é‡åŒ–åŸç†\n",
    "âœ… å¯¦ä½œ 4-bit BitsAndBytes é‡åŒ–\n",
    "âœ… è¨˜æ†¶é«”å„ªåŒ– ~75%\n",
    "\n",
    "### 2. Output Parser\n",
    "âœ… StrOutputParser - åŸºæœ¬å­—ä¸²\n",
    "âœ… CommaSeparatedListOutputParser - åˆ—è¡¨\n",
    "âœ… JsonOutputParser - JSON æ ¼å¼\n",
    "âœ… PydanticOutputParser - å‹åˆ¥å®‰å…¨ï¼ˆæ¨è–¦ï¼ï¼‰\n",
    "âœ… è‡ªå®šç¾© Parser\n",
    "\n",
    "### 3. RAG + Parser æ•´åˆ\n",
    "âœ… è«–æ–‡è³‡è¨Šæå–\n",
    "âœ… æŠ€è¡“å•ç­”ç³»çµ±\n",
    "âœ… æ–¹æ³•æ¯”è¼ƒåˆ†æ\n",
    "\n",
    "### 4. å•†æ¥­æ‡‰ç”¨\n",
    "âœ… æ™ºæ…§å®¢æœç³»çµ±\n",
    "âœ… éŒ¯èª¤è™•ç†èˆ‡é‡è©¦\n",
    "âœ… æ•ˆèƒ½å„ªåŒ–ç­–ç•¥\n",
    "\n",
    "## ğŸ’¼ å¯¦å‹™æŠ€èƒ½\n",
    "- è¨­è¨ˆçµæ§‹åŒ–è³‡æ–™æ¨¡å‹\n",
    "- æ•´åˆå¤šå€‹ LangChain çµ„ä»¶\n",
    "- è™•ç† LLM è¼¸å‡ºä¸ç¢ºå®šæ€§\n",
    "- å»ºç«‹å¯é çš„ç”Ÿç”¢ç³»çµ±\n",
    "\n",
    "## ğŸ“ ä¸‹ä¸€æ­¥å­¸ç¿’\n",
    "- Agent ç³»çµ±è¨­è¨ˆ\n",
    "- å¤šæ¨¡æ…‹ RAG\n",
    "- ç³»çµ±è©•ä¼°èˆ‡ç›£æ§\n",
    "- å¯¦éš›å°ˆæ¡ˆéƒ¨ç½²\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: ä½œæ¥­èªªæ˜\n",
    "print(\"\"\"ğŸ“ Week 6 ä½œæ¥­\n",
    "\n",
    "## åˆç´šä½œæ¥­ï¼ˆå¿…åšï¼‰\n",
    "\n",
    "### ä½œæ¥­ 1: Pydantic æ¨¡å‹è¨­è¨ˆ\n",
    "è¨­è¨ˆä¸€å€‹ `ProductReview` Pydantic æ¨¡å‹ï¼ŒåŒ…å«è‡³å°‘ 5 å€‹æ¬„ä½ï¼š\n",
    "- ç”¢å“åç¨±\n",
    "- è©•åˆ† (1-5)\n",
    "- è©•è«–å…§å®¹æ‘˜è¦\n",
    "- å„ªé»åˆ—è¡¨\n",
    "- ç¼ºé»åˆ—è¡¨\n",
    "\n",
    "### ä½œæ¥­ 2: é‡åŒ–æ•ˆèƒ½æ¯”è¼ƒ\n",
    "æ¯”è¼ƒé‡åŒ–å‰å¾Œçš„ï¼š\n",
    "- GPU è¨˜æ†¶é«”ä½¿ç”¨\n",
    "- æ¨ç†é€Ÿåº¦\n",
    "- è¼¸å‡ºå“è³ª\n",
    "\n",
    "### ä½œæ¥­ 3: åŸºç¤ RAG + Parser\n",
    "ä½¿ç”¨ JsonOutputParser å¾è«–æ–‡ä¸­æå–ï¼š\n",
    "- æ¨™é¡Œ\n",
    "- ä½œè€…\n",
    "- ä¸»è¦è²¢ç»\n",
    "\n",
    "## ä¸­ç´šä½œæ¥­ï¼ˆé¸åšï¼‰\n",
    "\n",
    "### ä½œæ¥­ 4: ç”¢å“æ¨è–¦ç³»çµ±\n",
    "å»ºç«‹çµæ§‹åŒ–ç”¢å“æ¨è–¦ç³»çµ±ï¼ŒåŒ…å«ï¼š\n",
    "- ç”¢å“è³‡è¨Š\n",
    "- æ¨è–¦ç†ç”±\n",
    "- é©ç”¨å ´æ™¯\n",
    "- ä¿¡å¿ƒåº¦è©•åˆ†\n",
    "\n",
    "### ä½œæ¥­ 5: æŠ€è¡“æ–‡ä»¶å•ç­”\n",
    "æ•´åˆ RAG + å¤šæ¬„ä½è¼¸å‡ºï¼š\n",
    "- å•é¡Œåˆ†é¡\n",
    "- è©³ç´°å›ç­”\n",
    "- ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼ˆå¦‚é©ç”¨ï¼‰\n",
    "- ç›¸é—œè³‡æºé€£çµ\n",
    "\n",
    "### ä½œæ¥­ 6: éŒ¯èª¤è™•ç†æ©Ÿåˆ¶\n",
    "å¯¦ä½œå®Œæ•´çš„éŒ¯èª¤è™•ç†ï¼š\n",
    "- é‡è©¦æ©Ÿåˆ¶ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰\n",
    "- é™ç´šæ–¹æ¡ˆ\n",
    "- éŒ¯èª¤æ—¥èªŒè¨˜éŒ„\n",
    "\n",
    "## é€²éšä½œæ¥­ï¼ˆæŒ‘æˆ°ï¼‰\n",
    "\n",
    "### ä½œæ¥­ 7: å¤šæ–‡ä»¶æª¢ç´¢ç³»çµ±\n",
    "ä½¿ç”¨ week04_rag/data/ ä¸­çš„æ‰€æœ‰è«–æ–‡ï¼š\n",
    "- å»ºç«‹è·¨æ–‡ä»¶æª¢ç´¢\n",
    "- å¯¦ä½œæ–‡ä»¶ä¾†æºæ¨™è¨»\n",
    "- æ¯”è¼ƒä¸åŒè«–æ–‡çš„è§€é»\n",
    "\n",
    "### ä½œæ¥­ 8: RAG å“è³ªè©•ä¼°\n",
    "å»ºç«‹è‡ªå‹•è©•ä¼°æ©Ÿåˆ¶ï¼š\n",
    "- ç­”æ¡ˆç›¸é—œæ€§è©•åˆ†\n",
    "- ä¾†æºå¯ä¿¡åº¦è©•ä¼°\n",
    "- è¼¸å‡ºæ ¼å¼å®Œæ•´æ€§æª¢æŸ¥\n",
    "\n",
    "### ä½œæ¥­ 9: å®Œæ•´å®¢æœç³»çµ±\n",
    "å»ºç«‹ç”Ÿç”¢ç´šå®¢æœçŸ¥è­˜åº«ï¼š\n",
    "- æ„åœ–åˆ†é¡\n",
    "- æƒ…æ„Ÿåˆ†æ\n",
    "- è‡ªå‹•å›è¦†\n",
    "- äººå·¥è½‰æ¥åˆ¤æ–·\n",
    "- æ»¿æ„åº¦è¿½è¹¤\n",
    "\n",
    "### ä½œæ¥­ 10: å¤§æ¨¡å‹é‡åŒ–å¯¦é©—\n",
    "å˜—è©¦ä½¿ç”¨ gemma-3-4b-itï¼š\n",
    "- å„ªåŒ–é‡åŒ–é…ç½®\n",
    "- æ¯”è¼ƒ 1B vs 4B æ•ˆæœ\n",
    "- åˆ†æè¨˜æ†¶é«”èˆ‡å“è³ªæ¬Šè¡¡\n",
    "\n",
    "## ğŸ“¤ ç¹³äº¤æ–¹å¼\n",
    "1. å®Œæˆçš„ Notebook (.ipynb)\n",
    "2. å¯¦é©—çµæœæˆªåœ–\n",
    "3. å¿ƒå¾—å ±å‘Šï¼ˆ500 å­—å…§ï¼‰\n",
    "\n",
    "## â° æˆªæ­¢æ™‚é–“\n",
    "ä¸‹é€±ä¸Šèª²å‰\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}