{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0-B5u6W7MR3Y"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "login(token=token)"
   ],
   "metadata": {
    "id": "0iirkOCuPvuh"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline"
   ],
   "metadata": {
    "id": "NMT1-f6uUmR9"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\")\n",
    "# pipe = pipeline(\"text-generation\", model=\"google/gemma-3-4b-it\")\n",
    "# messages= [{\"role\":\"user\",\"content\":\"What's Python?\"}]\n",
    "# response = pipe(messages)\n",
    "# print(response[0][\"generated_text\"][1][\"content\"])"
   ],
   "metadata": {
    "id": "ZMDd8h-nVxRU",
    "collapsed": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install langchain-huggingface"
   ],
   "metadata": {
    "id": "6Q7P3bUrelgv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/gemma-3-1b-it\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 5120,\n",
    "        \"temperature\": 0.8,}\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ],
   "metadata": {
    "id": "rx17XKNrX8HL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "65e67aa5c3ed4cedbebcfa0850321612",
      "024da84e70e640a284158457d102f84c",
      "49b49313ad28427a943de274315ee88b",
      "437007accf904c0dac14908d90e5a224",
      "2f04c977dc6f46bb8075364f62d89527",
      "2767797f6860414d8470397b1932497a",
      "e0899104accb4488b3227a1d474c581e",
      "31570092f4364be9b735cdc7f6a84df6",
      "a473e5291b404213a23a9c3b85dfa89b",
      "b3fc9e8c46754b1285084915f3dc1126",
      "33ee1935f0264bd6806a49c3d1d11ca7",
      "ff34bde1de1146b5b252a41b4ead30a9",
      "93a69601a6884cf6905d4daa4e5a8e5e",
      "9cff46215af64335be69849adae8061a",
      "8ba97993cd9f439c9fdd9225747948ce",
      "7229e8c2227b425da4259527b43d2f87",
      "1f0488a012df46969dac95a4d3b1b241",
      "77d038d308af40d0b999e8fe6cf5a807",
      "192d156a857c495e9659b9871b10c4c6",
      "f42308a390964f719433a1afde84ce3d",
      "9dff665e37e14c44ad93f7aadad15cc3",
      "b5a9e541529949ee98a054d212aec3d2",
      "0c3d7b43f6114c1bb9cc99227de4aea5",
      "44d04c3ec9124febb94651d8ca407ed6",
      "f846d5d6ecae490a842f12a5ded42076",
      "7ebf6a6aac6c4f5d90aab0edca1e2db7",
      "1b6668b437f34217a6bfa8946afc88e5",
      "20aa7520d6a44269a1d249b71bcb89c0",
      "dbd157814838475287848b63c3678cfd",
      "8458e2fbae6748d783bc2776cca54649",
      "418838b186414318881e5186997d661e",
      "af32eb50ebf14b80898cd44d2d55a6ae",
      "1351d0c1197c481891b0f07c3a666037",
      "82aceef206a14b9a838a573e430d2708",
      "f3282b6360ca486ab470ae52bbf1fc46",
      "f552c6ce15a54301ace0149806c2bbf1",
      "84e3e3f3cf61435c96e74efeb9a7ea5e",
      "0848260b64be4dfcadcece341fdccb2e",
      "6a735baaa8354f21bed1ad0f3577f24d",
      "a71070f0194e4e41847d22f7b090220d",
      "35d174cb586f4b6abc0325b9aa5dca48",
      "967be69fe3f14f6d9588c5a7fe471f9f",
      "39625b7573be4aa69e91b93b09344286",
      "0e9a00984c8e4fc99eeee27bc2a5fb32",
      "455c97ec53d54eee8913234a837718fd",
      "fd0f3606d9f449cebfec711892ae6462",
      "541420b8cf4a420787ac3467bcc1231c",
      "b3c1162bff514eb2931b44ec918f8980",
      "ba38f15125924b9da0dd9818d23c4eae",
      "73b45f43a70442d1b90820941bce771d",
      "547719c00f2b448b8d3ee57da17b6459",
      "c23caae27b804b7a8ffb4f80bd1e7500",
      "7fb903fed36044d1a2987c9e1e94413e",
      "a4f8ee1964634cf29f992365253e4149",
      "508afd1551fb43dda081483f1e316e89",
      "88ac852e57c94b9796ca671153499dec",
      "57ce7c938ede49b2bad640defec67daa",
      "e024cb8e09914f0f9247dc91a42ac538",
      "ad1e99d444d146859b09f6ae982325a7",
      "16b77f2cef4b45c29316de5c95d27349",
      "68d58fa9ae944920af9d680a9211cb06",
      "8ae2557dcd2548ffa2e1eceacdd7b7ce",
      "8928ff9f8dda40d8b7ed5215487e3b49",
      "218717de37554b5899f87e21c9e2c3cd",
      "285bd68b5a1e44d1b5032447d9feccb0",
      "152682eb53604496aeeae49eeb4a6e73",
      "0362a788ea1040ba88408807d9e5b6a5",
      "15d6efe834884e799fe77255b7069fc0",
      "e75f8829dde34c648e2a24af22db7d69",
      "e5da295a40f843e0872145dac3c78be7",
      "55c3156a71914238af3143a4acd3b44e",
      "d6647e41987f48378a70b6bc6f1b9d11",
      "b18b78d2e8f1445694e1129095d3780b",
      "3edd7818d6694bf2bf8132219d5b23d1",
      "129af1aed8024ba7bf5ff5b78fdc385c",
      "bdab0d6ce7304620abd930c264cdb7ac",
      "e61dedae8702487fb807b5f836fbefe3",
      "4abc1ee3c99c4e63aefbc0c5b00ab4d6",
      "2a8f2bd7e5a34777981b8b9bd4a724e2",
      "39e959a9670343669afe67f572d9396e",
      "ebcdab3176d8460d8d75b2c0b0fb8e87",
      "3c133c856b434911828be998c8d89fcc",
      "12d5936256134f51ba702ba00a298e99",
      "11206fcffbf74eec90dd242a62242003",
      "1a5fb9f3f3c7493bb99793fbbd6cc6ee",
      "95fde360e49a440a8477be088249c84b",
      "a315f01814ba4ccfb9375f9fcb13975c",
      "3648768e8ab148f1838f41559503a3ee"
     ]
    },
    "collapsed": true,
    "outputId": "06ddd013-d665-438a-9c13-2e9d2ad4d4b3"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65e67aa5c3ed4cedbebcfa0850321612"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff34bde1de1146b5b252a41b4ead30a9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c3d7b43f6114c1bb9cc99227de4aea5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82aceef206a14b9a838a573e430d2708"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "455c97ec53d54eee8913234a837718fd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88ac852e57c94b9796ca671153499dec"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0362a788ea1040ba88408807d9e5b6a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4abc1ee3c99c4e63aefbc0c5b00ab4d6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "messages = [SystemMessage(\"你是一個寫五言絕句，的寫詩的大師，請用繁體中文\"),\n",
    "            HumanMessage(\"請寫一首關於PYTHON的詩\")]\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qNDF2-zdOMi",
    "outputId": "24ca942f-4318-40db-c20d-c01fa7c660a8"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bos><start_of_turn>user\n",
      "你是一個寫五言絕句，的寫詩的大師，請用繁體中文\n",
      "\n",
      "請寫一首關於PYTHON的詩<end_of_turn>\n",
      "<start_of_turn>model\n",
      "```python\n",
      "夜幕星光閃，\n",
      "一行代码悄然落。\n",
      "數據流淌去，\n",
      "智能深思量。\n",
      "```\n",
      "\n",
      "**解析：**\n",
      "\n",
      "*   **夜幕星光閃：** 描寫了Python在夜晚的樣子，營造出神秘、閃爍的氛圍。\n",
      "*   **一行代码悄然落：** 點明主題，Python程式碼。\n",
      "*   **數據流淌去：**  將Python程式碼的運行過程，代表數據的流動。\n",
      "*   **智能深思量：**  表達Python的智慧和潛力，暗示其能力。\n",
      "\n",
      "希望你喜歡！\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gY51XlsAhzCu",
    "outputId": "7674ec9a-7275-445b-9ce4-b66e7914c70f"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd drive/MyDrive/data_rag"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFfq0CM_iBO2",
    "outputId": "1510a44e-0405-4fcf-cb58-3a52722e1683"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/data_rag\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install langchain_community pypdf"
   ],
   "metadata": {
    "collapsed": true,
    "id": "YwYqji2sqXa7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n"
   ],
   "metadata": {
    "id": "u4IlAhbNqiai"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loader = PyPDFLoader(\"2509.05396v1.pdf\")"
   ],
   "metadata": {
    "id": "Pa-zaWWdqyz7"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "doc = loader.load()"
   ],
   "metadata": {
    "id": "0r1vrvD2rE1C"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/gemma-3-1b-it\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 5120,\n",
    "        \"temperature\": 0.8,}\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "loader = PyPDFLoader(\"2509.05396v1.pdf\")\n",
    "doc = loader.load()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    ")\n",
    "\n",
    "# 1. 分割文件\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(doc)\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"根據以下資訊回答問題。如果資訊中沒有相關內容, 請說你不知道。\n",
    "\n",
    "相關資訊:\n",
    "{context}\n",
    "\n",
    "問題: {question}\n",
    "\n",
    "回答:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# 6. 建立 RAG Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "WdNQTnc0rW3y",
    "outputId": "b50943ca-2898-47bb-bfaa-aba3a766b14e"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install -U faiss-cpu==1.10.0"
   ],
   "metadata": {
    "id": "mX-4dfehVFx6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "result = qa_chain.invoke({\"query\": \"請問 multi agent debate 有沒有用?\"})"
   ],
   "metadata": {
    "id": "sY1SU0ciyVom"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "result[\"query\"]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WbQKAKupVaTa",
    "outputId": "0fd7a83c-4c72-4f8f-f8bf-3879e7d39366"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'請問 multi agent debate 有沒有用?'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(result[\"result\"])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOaUzZyxVpKj",
    "outputId": "0c68a9cb-2fa6-499e-c651-f761e7a772d8"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<bos><start_of_turn>user\n",
      "根據以下資訊回答問題。如果資訊中沒有相關內容, 請說你不知道。\n",
      "\n",
      "相關資訊:\n",
      "Talk Isn’t Always Cheap: Understanding Failure Modes in Multi-Agent Debate\n",
      "Andrea Wynn* 1 Harsh Satija * 2 Gillian Hadfield 1 3 2\n",
      "Abstract\n",
      "While multi-agent debate has been proposed as\n",
      "a promising strategy for improving AI reason-\n",
      "ing ability, we find that debate can sometimes be\n",
      "harmful rather than helpful. The prior work has\n",
      "exclusively focused on debates within homoge-\n",
      "neous groups of agents, whereas we explore how\n",
      "diversity in model capabilities influences the dy-\n",
      "\n",
      "tions (Irving et al., 2018; Khan et al., 2024; Michael et al.,\n",
      "2023; Kenton et al., 2024). More recently, another form of\n",
      "multi-agent debate, sometimes referred to as multi-agent\n",
      "deliberation, investigates at leveraging different LLM agents\n",
      "to surface better answers by having them exchange reason-\n",
      "ing via iterative discussion (Du et al., 2023; Chan et al.,\n",
      "2023; Liang et al., 2023; Subramaniam et al., 2025). Most\n",
      "of these studies focus on a homogeneous setting where all\n",
      "\n",
      "new line of research on interactive reasoning among mul-\n",
      "tiple LLMs through debate has promoted the multi-agent\n",
      "debate framework as a promising approach to enhancing\n",
      "the reasoning and decision-making capabilities of LLM\n",
      "agents (Du et al., 2023; Chan et al., 2023; Liang et al., 2023;\n",
      "Khan et al., 2024). Various forms of multi-agent debate\n",
      "have been shown to improve performance on multiple arith-\n",
      "*Equal contribution 1Department of Computer Science, Johns\n",
      "\n",
      "isn’t always cheap – and in some cases, it’s actively harmful.\n",
      "We present a systematic evaluation of multi-agent debate\n",
      "across multiple tasks, showing that debate can sometimes\n",
      "harm group performance, particularly with heterogeneous\n",
      "LLM agents engaged in debate. Our findings challenge the\n",
      "prevailing narrative that more discussion between agents is\n",
      "inherently beneficial. Instead, we uncover several key fac-\n",
      "tors that mediate the success or failure of debate, including\n",
      "\n",
      "Talk Isn’t Always Cheap: Understanding Failure Modes in Multi-Agent Debate\n",
      "helps – and when it hurts. Together, these results suggest\n",
      "that while debate remains a promising tool for improving\n",
      "model reasoning, its success is far from guaranteed.\n",
      "2. Related Work\n",
      "Debate and multi-agent reasoning. Multi-agent debate\n",
      "was initially proposed as method for scalable oversight prob-\n",
      "lem where a judge or verifier can interject and elicit hidden\n",
      "contradictions, using structured back-and-forth conversa-\n",
      "\n",
      "問題: 請問 multi agent debate 有沒有用?\n",
      "\n",
      "回答:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "是的，multi-agent debate 可以有用。\n",
      "\n",
      "根據文章，multi-agent debate 并非总是有帮助，有时甚至有害。文章指出，在多agent辩论中，不同的模型能力可能导致不利于结果。\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Mweg4sfCVtJJ"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}