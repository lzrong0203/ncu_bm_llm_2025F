#!/usr/bin/env python3
"""
Week 1 - Lesson 3: Ollama API åŸºç¤
æ·±å…¥äº†è§£ Ollama API çš„å„ç¨®åŠŸèƒ½
"""

import ollama
import json
import time
from typing import Dict, List, Any

class OllamaExplorer:
    """Ollama API æ¢ç´¢å·¥å…·"""
    
    def __init__(self):
        self.client = ollama
    
    def list_models(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å·²å®‰è£çš„æ¨¡å‹"""
        print("\nğŸ“¦ å·²å®‰è£çš„æ¨¡å‹ï¼š")
        print("=" * 60)
        
        models = self.client.list()
        model_list = []
        
        for model in models['models']:
            size_mb = model['size'] / (1024 * 1024)
            modified = model['modified_at']
            
            print(f"ğŸ“Œ {model['name']}")
            print(f"   å¤§å°: {size_mb:.1f} MB")
            print(f"   ä¿®æ”¹æ™‚é–“: {modified}")
            print(f"   æ‘˜è¦: {model['digest'][:20]}...")
            print("-" * 40)
            
            model_list.append({
                'name': model['name'],
                'size_mb': size_mb,
                'modified': modified
            })
        
        return model_list
    
    def show_model_info(self, model_name: str) -> Dict:
        """é¡¯ç¤ºæ¨¡å‹è©³ç´°è³‡è¨Š"""
        print(f"\nğŸ” æ¨¡å‹è³‡è¨Š: {model_name}")
        print("=" * 60)
        
        try:
            info = self.client.show(model_name)
            
            # åŸºæœ¬è³‡è¨Š
            print("ğŸ“Š åŸºæœ¬è³‡è¨Š:")
            print(f"   æ¨¡å‹å®¶æ—: {info.get('details', {}).get('family', 'N/A')}")
            print(f"   åƒæ•¸é‡: {info.get('details', {}).get('parameter_size', 'N/A')}")
            print(f"   é‡åŒ–ç­‰ç´š: {info.get('details', {}).get('quantization_level', 'N/A')}")
            
            # æ¨¡å‹å¡ç‰‡ (å¦‚æœæœ‰çš„è©±)
            if 'modelfile' in info:
                print("\nğŸ“ Modelfile ç‰‡æ®µ:")
                lines = info['modelfile'].split('\n')[:5]
                for line in lines:
                    print(f"   {line}")
            
            # æˆæ¬Šè³‡è¨Š
            if 'license' in info:
                print(f"\nğŸ“œ æˆæ¬Š: {info['license'][:100]}...")
            
            return info
            
        except Exception as e:
            print(f"âŒ ç„¡æ³•ç²å–æ¨¡å‹è³‡è¨Š: {e}")
            return {}
    
    def test_generate_api(self, model: str = "gemma:2b") -> None:
        """æ¸¬è©¦ generate API (åŸå§‹æ–‡å­—ç”Ÿæˆ)"""
        print(f"\nğŸ”¬ æ¸¬è©¦ Generate API - {model}")
        print("=" * 60)
        
        prompts = [
            "å®Œæˆé€™å€‹å¥å­ï¼šäººå·¥æ™ºæ…§çš„æœªä¾†",
            "å¯«ä¸€å€‹é—œæ–¼æ©Ÿå™¨äººçš„çŸ­æ•…äº‹ï¼Œ50å­—ä»¥å…§ï¼š",
            "è§£é‡‹ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’ï¼š"
        ]
        
        for prompt in prompts:
            print(f"\nğŸ“ Prompt: {prompt}")
            
            start_time = time.time()
            response = self.client.generate(
                model=model,
                prompt=prompt,
                options={
                    'temperature': 0.7,
                    'max_tokens': 100
                }
            )
            elapsed = time.time() - start_time
            
            print(f"ğŸ’¬ Response: {response['response']}")
            print(f"â±ï¸  ç”Ÿæˆæ™‚é–“: {elapsed:.2f} ç§’")
            
            if 'context' in response:
                print(f"ğŸ“Š Context é•·åº¦: {len(response['context'])}")
    
    def test_chat_api(self, model: str = "gemma:2b") -> None:
        """æ¸¬è©¦ chat API (å°è©±æ¨¡å¼)"""
        print(f"\nğŸ”¬ æ¸¬è©¦ Chat API - {model}")
        print("=" * 60)
        
        # å¤šè¼ªå°è©±æ¸¬è©¦
        messages = [
            {'role': 'system', 'content': 'ä½ æ˜¯ä¸€å€‹å‹å–„çš„åŠ©ç†ï¼Œç”¨ç¹é«”ä¸­æ–‡å›ç­”'},
            {'role': 'user', 'content': 'è«‹è¨˜ä½æˆ‘æœ€å–œæ­¡çš„é¡è‰²æ˜¯è—è‰²'},
            {'role': 'assistant', 'content': 'å¥½çš„ï¼Œæˆ‘è¨˜ä½äº†ä½ æœ€å–œæ­¡çš„é¡è‰²æ˜¯è—è‰²ã€‚'},
            {'role': 'user', 'content': 'æˆ‘æœ€å–œæ­¡ä»€éº¼é¡è‰²ï¼Ÿ'}
        ]
        
        print("ğŸ“ å°è©±æ­·å²ï¼š")
        for msg in messages:
            role_icon = "ğŸ¤–" if msg['role'] == 'assistant' else "ğŸ‘¤"
            if msg['role'] != 'system':
                print(f"{role_icon} {msg['role']}: {msg['content']}")
        
        print("\næ­£åœ¨ç”Ÿæˆå›æ‡‰...")
        response = self.client.chat(
            model=model,
            messages=messages
        )
        
        print(f"ğŸ¤– åŠ©ç†: {response['message']['content']}")
        
        # é¡¯ç¤º token ä½¿ç”¨æƒ…æ³
        if 'prompt_eval_count' in response:
            print(f"\nğŸ“Š Token çµ±è¨ˆ:")
            print(f"   è¼¸å…¥ tokens: {response.get('prompt_eval_count', 0)}")
            print(f"   è¼¸å‡º tokens: {response.get('eval_count', 0)}")
    
    def test_embeddings(self, model: str = "gemma:2b") -> None:
        """æ¸¬è©¦ embeddings API"""
        print(f"\nğŸ”¬ æ¸¬è©¦ Embeddings API - {model}")
        print("=" * 60)
        
        texts = [
            "æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯",
            "æ·±åº¦å­¸ç¿’ä½¿ç”¨ç¥ç¶“ç¶²è·¯",
            "ä»Šå¤©å¤©æ°£å¾ˆå¥½"
        ]
        
        embeddings = []
        for text in texts:
            print(f"\nğŸ“ æ–‡å­—: {text}")
            
            try:
                response = self.client.embeddings(
                    model=model,
                    prompt=text
                )
                
                embedding = response['embedding']
                embeddings.append(embedding)
                
                print(f"   å‘é‡ç¶­åº¦: {len(embedding)}")
                print(f"   å‰5å€‹å€¼: {embedding[:5]}")
                
            except Exception as e:
                print(f"   âŒ éŒ¯èª¤: {e}")
        
        # è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆé¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰
        if len(embeddings) >= 2:
            print("\nğŸ“ ç›¸ä¼¼åº¦åˆ†æï¼š")
            import math
            
            def cosine_similarity(v1, v2):
                dot_product = sum(a * b for a, b in zip(v1, v2))
                norm1 = math.sqrt(sum(a * a for a in v1))
                norm2 = math.sqrt(sum(b * b for b in v2))
                return dot_product / (norm1 * norm2)
            
            for i in range(len(texts)):
                for j in range(i + 1, len(texts)):
                    similarity = cosine_similarity(embeddings[i], embeddings[j])
                    print(f"   '{texts[i][:20]}...' vs '{texts[j][:20]}...': {similarity:.3f}")
    
    def benchmark_performance(self, model: str = "gemma:2b") -> Dict:
        """æ•ˆèƒ½åŸºæº–æ¸¬è©¦"""
        print(f"\nâš¡ æ•ˆèƒ½åŸºæº–æ¸¬è©¦ - {model}")
        print("=" * 60)
        
        test_prompts = [
            "Hello, how are you?",
            "å¯«ä¸€å€‹10å­—çš„å¥å­",
            "1+1ç­‰æ–¼å¤šå°‘ï¼Ÿ"
        ]
        
        results = []
        
        for prompt in test_prompts:
            print(f"\næ¸¬è©¦: {prompt}")
            
            # æ¸¬è©¦ç”Ÿæˆé€Ÿåº¦
            start = time.time()
            response = self.client.generate(
                model=model,
                prompt=prompt,
                options={'max_tokens': 50}
            )
            elapsed = time.time() - start
            
            tokens = response.get('eval_count', 0)
            tokens_per_sec = tokens / elapsed if elapsed > 0 else 0
            
            print(f"   æ™‚é–“: {elapsed:.2f}ç§’")
            print(f"   Tokens: {tokens}")
            print(f"   é€Ÿåº¦: {tokens_per_sec:.1f} tokens/ç§’")
            
            results.append({
                'prompt': prompt,
                'time': elapsed,
                'tokens': tokens,
                'tokens_per_sec': tokens_per_sec
            })
        
        # ç¸½çµ
        avg_speed = sum(r['tokens_per_sec'] for r in results) / len(results)
        print(f"\nğŸ“Š å¹³å‡é€Ÿåº¦: {avg_speed:.1f} tokens/ç§’")
        
        return {
            'model': model,
            'average_tokens_per_sec': avg_speed,
            'results': results
        }
    
    def test_model_options(self, model: str = "gemma:2b") -> None:
        """æ¸¬è©¦å„ç¨®æ¨¡å‹é¸é …"""
        print(f"\nğŸ›ï¸ æ¸¬è©¦æ¨¡å‹é¸é … - {model}")
        print("=" * 60)
        
        prompt = "å¯«ä¸€å€‹é—œæ–¼ AI çš„å¥å­"
        
        # æ¸¬è©¦ä¸åŒçš„é¸é …çµ„åˆ
        options_list = [
            {'temperature': 0.1, 'seed': 42},
            {'temperature': 0.9, 'seed': 42},
            {'temperature': 0.5, 'top_p': 0.9},
            {'temperature': 0.5, 'top_k': 40},
            {'temperature': 0.5, 'repeat_penalty': 1.5}
        ]
        
        for i, options in enumerate(options_list, 1):
            print(f"\næ¸¬è©¦ {i}: {options}")
            response = self.client.generate(
                model=model,
                prompt=prompt,
                options=options
            )
            print(f"   çµæœ: {response['response']}")

def print_menu():
    """é¡¯ç¤ºé¸å–®"""
    print("\n" + "=" * 60)
    print("ğŸ“š Ollama API æ¢ç´¢å·¥å…·")
    print("=" * 60)
    print("1. åˆ—å‡ºå·²å®‰è£æ¨¡å‹")
    print("2. é¡¯ç¤ºæ¨¡å‹è©³ç´°è³‡è¨Š")
    print("3. æ¸¬è©¦ Generate API")
    print("4. æ¸¬è©¦ Chat API")
    print("5. æ¸¬è©¦ Embeddings")
    print("6. æ•ˆèƒ½åŸºæº–æ¸¬è©¦")
    print("7. æ¸¬è©¦æ¨¡å‹é¸é …")
    print("8. åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦")
    print("0. çµæŸ")
    print("-" * 60)

def main():
    """ä¸»ç¨‹å¼"""
    explorer = OllamaExplorer()
    
    # æª¢æŸ¥é€£æ¥
    try:
        models = ollama.list()
        if not models['models']:
            print("âš ï¸  æ²’æœ‰æ‰¾åˆ°å·²å®‰è£çš„æ¨¡å‹")
            print("è«‹å…ˆå®‰è£æ¨¡å‹ï¼šollama pull gemma:2b")
            return
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Ollama: {e}")
        return
    
    default_model = "gemma:2b"
    
    while True:
        print_menu()
        
        choice = input("è«‹é¸æ“‡ (0-8): ").strip()
        
        if choice == '0':
            print("ğŸ‘‹ å†è¦‹ï¼")
            break
        
        elif choice == '1':
            explorer.list_models()
        
        elif choice == '2':
            model = input(f"è¼¸å…¥æ¨¡å‹åç¨± [{default_model}]: ").strip() or default_model
            explorer.show_model_info(model)
        
        elif choice == '3':
            model = input(f"è¼¸å…¥æ¨¡å‹åç¨± [{default_model}]: ").strip() or default_model
            explorer.test_generate_api(model)
        
        elif choice == '4':
            model = input(f"è¼¸å…¥æ¨¡å‹åç¨± [{default_model}]: ").strip() or default_model
            explorer.test_chat_api(model)
        
        elif choice == '5':
            model = input(f"è¼¸å…¥æ¨¡å‹åç¨± [{default_model}]: ").strip() or default_model
            explorer.test_embeddings(model)
        
        elif choice == '6':
            model = input(f"è¼¸å…¥æ¨¡å‹åç¨± [{default_model}]: ").strip() or default_model
            explorer.benchmark_performance(model)
        
        elif choice == '7':
            model = input(f"è¼¸å…¥æ¨¡å‹åç¨± [{default_model}]: ").strip() or default_model
            explorer.test_model_options(model)
        
        elif choice == '8':
            model = input(f"è¼¸å…¥æ¨¡å‹åç¨± [{default_model}]: ").strip() or default_model
            print("\nğŸš€ åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦...")
            explorer.list_models()
            explorer.show_model_info(model)
            explorer.test_generate_api(model)
            explorer.test_chat_api(model)
            explorer.test_embeddings(model)
            explorer.benchmark_performance(model)
            explorer.test_model_options(model)
            print("\nâœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
        
        else:
            print("âŒ ç„¡æ•ˆçš„é¸æ“‡")
        
        input("\næŒ‰ Enter ç¹¼çºŒ...")

if __name__ == "__main__":
    main()