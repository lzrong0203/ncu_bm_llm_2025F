#!/usr/bin/env python3
"""
Week 1 - Lesson 1: Hello LLM
ç¬¬ä¸€å€‹æœ¬åœ° LLM ç¨‹å¼
"""

import ollama
import sys
from typing import Optional

def check_ollama_installation() -> bool:
    """æª¢æŸ¥ Ollama æ˜¯å¦å·²å®‰è£ä¸¦é‹è¡Œ"""
    try:
        models = ollama.list()
        print("âœ… Ollama å·²æˆåŠŸé€£æ¥ï¼")
        print(f"ğŸ“¦ å·²å®‰è£çš„æ¨¡å‹ï¼š")
        for model in models['models']:
            print(f"   - {model['name']} ({model['size'] / 1024 / 1024:.1f} MB)")
        return True
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Ollama: {e}")
        print("\nè«‹ç¢ºèªï¼š")
        print("1. Ollama å·²å®‰è£ï¼šhttps://ollama.com/download")
        print("2. Ollama æœå‹™æ­£åœ¨é‹è¡Œï¼šollama serve")
        return False

def download_model(model_name: str = "gemma2:9b-instruct-q4_0") -> bool:
    """ä¸‹è¼‰æŒ‡å®šçš„æ¨¡å‹"""
    try:
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è¼‰æ¨¡å‹ {model_name}...")
        ollama.pull(model_name)
        print(f"âœ… æ¨¡å‹ {model_name} ä¸‹è¼‰å®Œæˆï¼")
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        return False

def simple_chat(model: str = "gemma2:9b-instruct-q4_0", prompt: str = None) -> None:
    """ç°¡å–®çš„å°è©±åŠŸèƒ½"""
    if prompt is None:
        prompt = "ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±ï¼Œç”¨ç¹é«”ä¸­æ–‡å›ç­”"
    
    print(f"\nğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
    print(f"ğŸ“ æç¤ºè©: {prompt}")
    print("-" * 50)
    
    try:
        # æ–¹æ³•1: ç°¡å–®å°è©±ï¼ˆä¸€æ¬¡æ€§å›æ‡‰ï¼‰
        response = ollama.chat(
            model=model,
            messages=[{
                'role': 'user',
                'content': prompt
            }]
        )
        print("ğŸ’¬ å›æ‡‰ï¼š")
        print(response['message']['content'])
        
        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        if 'total_duration' in response:
            duration_sec = response['total_duration'] / 1_000_000_000
            print(f"\nâ±ï¸  ç”Ÿæˆæ™‚é–“: {duration_sec:.2f} ç§’")
        
    except Exception as e:
        print(f"âŒ å°è©±å¤±æ•—: {e}")

def streaming_chat(model: str = "gemma2:9b-instruct-q4_0", prompt: str = None) -> None:
    """ä¸²æµå°è©±ï¼ˆæ‰“å­—æ©Ÿæ•ˆæœï¼‰"""
    if prompt is None:
        prompt = "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿè«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹"
    
    print(f"\nğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
    print(f"ğŸ“ æç¤ºè©: {prompt}")
    print("-" * 50)
    print("ğŸ’¬ å›æ‡‰ï¼š")
    
    try:
        # æ–¹æ³•2: ä¸²æµå›æ‡‰ï¼ˆæ‰“å­—æ©Ÿæ•ˆæœï¼‰
        stream = ollama.chat(
            model=model,
            messages=[{
                'role': 'user',
                'content': prompt
            }],
            stream=True
        )
        
        for chunk in stream:
            print(chunk['message']['content'], end='', flush=True)
        print("\n")
        
    except Exception as e:
        print(f"âŒ ä¸²æµå°è©±å¤±æ•—: {e}")

def explore_model_parameters(model: str = "gemma2:9b-instruct-q4_0") -> None:
    """æ¢ç´¢æ¨¡å‹åƒæ•¸çš„å½±éŸ¿"""
    prompt = "å¯«ä¸€å¥é—œæ–¼ AI çš„å¥å­"
    temperatures = [0.1, 0.5, 0.9]
    
    print(f"\nğŸ”¬ å¯¦é©—ï¼šTemperature å°è¼¸å‡ºçš„å½±éŸ¿")
    print(f"ğŸ“ æç¤ºè©: {prompt}")
    print("=" * 50)
    
    for temp in temperatures:
        print(f"\nğŸŒ¡ï¸  Temperature = {temp}")
        response = ollama.generate(
            model=model,
            prompt=prompt,
            options={
                'temperature': temp,
                'seed': 42  # å›ºå®šç¨®å­ä»¥ä¾¿æ¯”è¼ƒ
            }
        )
        print(f"   {response['response']}")

def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 50)
    print("ğŸš€ Week 1 - Lesson 1: Hello LLM")
    print("=" * 50)
    
    # Step 1: æª¢æŸ¥ Ollama
    if not check_ollama_installation():
        sys.exit(1)
    
    # Step 2: ç¢ºèªæ¨¡å‹ï¼ˆæ”¯æ´å¤šç¨®é¸æ“‡ï¼‰
    print("\nğŸ¤– å¯ç”¨æ¨¡å‹é¸é …ï¼š")
    print("1. gemma2:2b - è¶…è¼•é‡ (1.4GB, 8GB RAM)")
    print("2. gemma2:9b-instruct-q4_0 - æ¨è–¦ (5.5GB, 16GB RAM)")
    print("3. llama3.2:3b - Meta æœ€æ–° (2GB, 8GB RAM)")
    print("4. qwen2.5:7b - ä¸­æ–‡æœ€å¼· (4.7GB, 16GB RAM)")
    
    choice = input("\né¸æ“‡æ¨¡å‹ (1-4) [é è¨­: 2]: ").strip()
    
    model_options = {
        "1": "gemma2:2b",
        "2": "gemma2:9b-instruct-q4_0",
        "3": "llama3.2:3b",
        "4": "qwen2.5:7b"
    }
    
    model_name = model_options.get(choice, "gemma2:9b-instruct-q4_0")
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
    models = ollama.list()
    model_exists = any(model_name in m['name'] for m in models['models'])
    
    if not model_exists:
        print(f"\nâš ï¸  æ¨¡å‹ {model_name} æœªå®‰è£")
        response = input("æ˜¯å¦è¦ä¸‹è¼‰ï¼Ÿ(y/n): ")
        if response.lower() == 'y':
            if not download_model(model_name):
                sys.exit(1)
        else:
            print(f"è«‹å…ˆä¸‹è¼‰æ¨¡å‹ï¼šollama pull {model_name}")
            sys.exit(1)
    
    # Step 3: åŸ·è¡Œä¸åŒçš„å°è©±æ¨¡å¼
    print("\n" + "=" * 50)
    print("ğŸ“š ç¤ºç¯„ 1: ç°¡å–®å°è©±")
    print("=" * 50)
    simple_chat(model_name)
    
    input("\næŒ‰ Enter ç¹¼çºŒ...")
    
    print("\n" + "=" * 50)
    print("ğŸ“š ç¤ºç¯„ 2: ä¸²æµå°è©±")
    print("=" * 50)
    streaming_chat(model_name)
    
    input("\næŒ‰ Enter ç¹¼çºŒ...")
    
    print("\n" + "=" * 50)
    print("ğŸ“š ç¤ºç¯„ 3: Temperature å¯¦é©—")
    print("=" * 50)
    explore_model_parameters(model_name)
    
    print("\nâœ… èª²ç¨‹å®Œæˆï¼")
    print("ğŸ“ ç·´ç¿’å»ºè­°ï¼š")
    print("1. å˜—è©¦ä¸åŒçš„æç¤ºè©")
    print("2. æ¸¬è©¦ä¸åŒçš„ temperature å€¼")
    print("3. æ¯”è¼ƒä¸åŒæ¨¡å‹çš„å›æ‡‰é€Ÿåº¦å’Œå“è³ª")

if __name__ == "__main__":
    main()